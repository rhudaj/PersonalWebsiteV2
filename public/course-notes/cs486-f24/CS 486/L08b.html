<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>L08b </title><link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/></head><body><article id="13d2e7d3-01e5-80db-8850-c6457f397935" class="page sans"><header><div class="page-header-icon undefined"><img class="icon" src="https://www.notion.so/icons/document_pink.svg"/></div><h1 class="page-title">L08b </h1><p class="page-description"></p></header><div class="page-body"><p id="13d2e7d3-01e5-809d-a274-c5efffffe4ca" class=""><strong>topic: </strong>supervised ML </p><h1 id="13e2e7d3-01e5-80c8-8628-ff0f9cc12ad1" class="">Neural Networks</h1><p id="13e2e7d3-01e5-802a-8313-d242514efb96" class="">Essentially, it’s one big composite function.</p><ul id="13e2e7d3-01e5-80b3-8b27-d527a7bdd24c" class="toggle"><li><details open=""><summary>Similar to a Decision Tree</summary><ul id="13e2e7d3-01e5-80c9-9b1f-c565be6a4672" class="bulleted-list"><li style="list-style-type:disc">Both models are capable of representing complex decision boundaries and can classify or predict outputs based on the features provided in the input. A decision tree learns through a series of simple, hierarchical if-then rules that split the data into increasingly refined groups. Neural networks, on the other hand, use layers of connected “neurons” that transform the data non-linearly. </li></ul><ul id="13e2e7d3-01e5-8074-b170-c3a8064f9f32" class="bulleted-list"><li style="list-style-type:disc"><strong>Decision Tree Bias:</strong> Decision trees have a bias toward learning hierarchical, rule-based relationships. They work by recursively splitting the feature space, which tends to yield decisions that are easy to interpret. This means decision trees might be well-suited for problems where the data naturally divides into clear, logical segments.</li></ul><ul id="13e2e7d3-01e5-8069-bbbd-e6e1513ffb95" class="bulleted-list"><li style="list-style-type:disc"><strong>Neural Network Bias:</strong> Neural networks have a bias toward learning continuous, non-linear relationships, which allows them to represent more complex, subtle patterns. Since they rely on weighted connections and transformations across multiple layers, they can capture nuanced interactions between features that aren’t easily expressed in simple rules. This is particularly useful in applications like image or speech recognition, where patterns are complex and continuous rather than discrete.</li></ul><ul id="13e2e7d3-01e5-80c2-825c-c0927bcc098d" class="bulleted-list"><li style="list-style-type:disc">In essence, although both models can learn from the same data, the <em>type of patterns</em> they are predisposed to learn differs because of their inherent structures.</li></ul></details></li></ul><ul id="13e2e7d3-01e5-80cd-8045-d25854b2d151" class="toggle"><li><details open=""><summary>Activation Functions </summary><p id="1562e7d3-01e5-80a9-b571-e6b99286138d" class="">The <mark class="highlight-orange">activation function</mark> should be <em>non-linear</em>, otherwise it’s a linear combination, so might as use one function. </p><p id="1562e7d3-01e5-80ae-9514-fe5dd1a2df18" class="">Common Functions: </p><ul id="1562e7d3-01e5-8038-bf22-c39bba7ed293" class="bulleted-list"><li style="list-style-type:disc">Step Function: usefull for binary classification</li></ul><figure id="13e2e7d3-01e5-802a-a42d-d92956edd13b" class="image" style="text-align:center"><a href="L08b/Screenshot_2024-11-13_at_19.50.59.png"><img style="width:384px" src="L08b/Screenshot_2024-11-13_at_19.50.59.png"/></a></figure><ul id="1562e7d3-01e5-8021-8d70-caaec5ca0135" class="bulleted-list"><li style="list-style-type:disc">Sigmoid Function </li></ul><figure id="13e2e7d3-01e5-802d-acac-cb635dcf112d" class="image" style="text-align:center"><a href="L08b/Screenshot_2024-11-13_at_19.51.06.png"><img style="width:432px" src="L08b/Screenshot_2024-11-13_at_19.51.06.png"/></a></figure><ul id="1562e7d3-01e5-8056-9fb1-d81ac37e3559" class="bulleted-list"><li style="list-style-type:disc">ReLU</li></ul><figure id="13e2e7d3-01e5-8069-9b05-c3765b5daa0f" class="image" style="text-align:center"><a href="L08b/Screenshot_2024-11-13_at_19.51.16.png"><img style="width:432px" src="L08b/Screenshot_2024-11-13_at_19.51.16.png"/></a></figure><ul id="1562e7d3-01e5-80ef-85a6-f39b289d6a71" class="bulleted-list"><li style="list-style-type:disc">Leaky ReLU</li></ul><figure id="13e2e7d3-01e5-80e8-bb6c-c097181f27c8" class="image" style="text-align:center"><a href="L08b/Screenshot_2024-11-13_at_19.51.24.png"><img style="width:432px" src="L08b/Screenshot_2024-11-13_at_19.51.24.png"/></a></figure></details></li></ul><p id="13e2e7d3-01e5-80b2-a76d-d8e7b53ead8f" class="">We will only consider <em><strong><span style="border-bottom:0.05em solid">Feedforward Networks</span></strong></em></p><ul id="13e2e7d3-01e5-808a-abaf-c1f199d6935d" class="bulleted-list"><li style="list-style-type:disc">No loops. Connections only in one direction. </li></ul><ul id="13e2e7d3-01e5-809f-918b-d68f631dee8b" class="bulleted-list"><li style="list-style-type:disc">The output is a (composite) function of its inputs. </li></ul><p id="1562e7d3-01e5-804b-bcc7-f3a9d18d0a2c" class=""><strong>Note: </strong>the <strong># nodes in the output layer = dimensionality of the target vector </strong></p><hr id="1562e7d3-01e5-8038-b129-e6e6f815dd08"/><figure id="1562e7d3-01e5-8003-87f7-c969cac70143" class="image"><a href="L08b/IMG_4837B3F61E3C-1.jpeg"><img style="width:624px" src="L08b/IMG_4837B3F61E3C-1.jpeg"/></a></figure><hr id="1562e7d3-01e5-8097-a048-e5da839cf592"/><ul id="13e2e7d3-01e5-8061-9d15-cd9c23501a36" class="toggle"><li><details open=""><summary><mark class="highlight-purple"><strong>Example — Discussion Board </strong></mark></summary><figure id="13e2e7d3-01e5-801e-be4b-eab24237bc63" class="image"><a href="L08b/IMG_F931431C9C90-1.jpeg"><img style="width:480px" src="L08b/IMG_F931431C9C90-1.jpeg"/></a></figure></details></li></ul><hr id="13e2e7d3-01e5-8020-89fe-cd1bc49c72bf"/><h2 id="13e2e7d3-01e5-8037-b6ac-ef55444c3f62" class="">Back Propagation</h2><ul id="13e2e7d3-01e5-8050-bd5a-e6b3bbff4963" class="bulleted-list"><li style="list-style-type:disc">Gradient Descent across layers. </li></ul><ul id="1562e7d3-01e5-80af-a84b-d874ada4fc9b" class="bulleted-list"><li style="list-style-type:disc"><strong>Why its a Challenge? </strong><ul id="1562e7d3-01e5-80b6-a581-fbc8eb8eb1e5" class="bulleted-list"><li style="list-style-type:circle">In a Neural Network, each node contributes to the overall error at the output node. </li></ul><ul id="13e2e7d3-01e5-80af-89f1-e4f2fe907639" class="bulleted-list"><li style="list-style-type:circle">With multiple layers, finding out which weight contributed to the error is more challenging.</li></ul></li></ul><ul id="1562e7d3-01e5-80c9-abc7-fa67badc91d8" class="bulleted-list"><li style="list-style-type:disc"><strong>Goal: </strong>Given training examples <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo separator="true">,</mo><mi>Y</mi></mrow><annotation encoding="application/x-tex">X,Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span></span></span></span><span>﻿</span></span> and error function, perform <strong>2 passes: </strong><ol type="1" id="13e2e7d3-01e5-80b4-844f-efb3fde021f2" class="numbered-list" start="1"><li><strong>Forward Pass: </strong>Compute the error at the output.</li></ol><ol type="1" id="1562e7d3-01e5-80a0-9cc7-cb235625fa04" class="numbered-list" start="2"><li><strong>Backward Pass: </strong>Compute the errors with w.r.t to the weights at each layer.</li></ol></li></ul><h3 id="1412e7d3-01e5-80d5-ba90-d8533880ced3" class="">Backward Pass</h3><p id="1422e7d3-01e5-807e-8adf-e057db864bcf" class=""><em>Compute delta’s recursively, starting from the last layer</em></p><figure id="1562e7d3-01e5-8031-a5f7-cd197d7e994d" class="image"><a href="L08b/IMG_51AB4E8DC1F3-1.jpeg"><img style="width:707.998046875px" src="L08b/IMG_51AB4E8DC1F3-1.jpeg"/></a></figure><ul id="13e2e7d3-01e5-8060-ac75-da69d934364e" class="toggle"><li><details open=""><summary><mark class="highlight-purple"><strong>Detailed Example </strong></mark></summary><p id="13e2e7d3-01e5-8026-8738-e66c4e6f9337" class="">2 Layers, 2 Neurons </p><figure id="13e2e7d3-01e5-8009-b759-efb4b812fcef" class="image" style="text-align:center"><a href="L08b/IMG_CECBCCF72DD2-1.jpeg"><img style="width:432px" src="L08b/IMG_CECBCCF72DD2-1.jpeg"/></a><figcaption><strong>Error: </strong>should be <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>z</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">z_j^{(1)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.4578em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> for step #1</figcaption></figure></details></li></ul><p id="1422e7d3-01e5-8030-8a11-edf1742217d2" class="">
</p><p id="1422e7d3-01e5-8069-82af-d8b75c9f30af" class=""><strong>Note — How to handle bias terms? </strong></p><ul id="1422e7d3-01e5-8025-a0ec-ea5a26c85053" class="bulleted-list"><li style="list-style-type:disc">You update the *weights* on the bias units - these are (for the 2-layer NN we studied in class), <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>W</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>1</mn></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">W^{(i)}_{0,1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.4472em;vertical-align:-0.4024em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4337em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4024em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> and <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>W</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>2</mn></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">W^{(i)}_{0,2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.4472em;vertical-align:-0.4024em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4337em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span></span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4024em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> where <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>∈</mo><mo stretchy="false">{</mo><mtext> </mtext><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mtext> </mtext><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">i\in \set{1,2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6986em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mclose">}</span></span></span></span></span><span>﻿</span></span>. The actual bias terms are <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> and <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>z</mi><mn>0</mn><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">z_0^{(1)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3111em;vertical-align:-0.2663em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4337em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2663em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> and these are always <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></span><span>﻿</span></span>. </li></ul><ul id="1562e7d3-01e5-80b7-b6cd-ee8ae4506749" class="bulleted-list"><li style="list-style-type:disc">When back-propagating, you don’t include <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>W</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>1</mn></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">W^{(i)}_{0,1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.4472em;vertical-align:-0.4024em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4337em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4024em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> and <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>W</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>2</mn></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">W^{(i)}_{0,2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.4472em;vertical-align:-0.4024em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4337em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span></span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4024em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span>  when computing <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mover accent="true"><mi>δ</mi><mo>⃗</mo></mover><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\vec\delta^{(1)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9774em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9774em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03785em;">δ</span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1799em;"><span class="overlay" style="height:0.714em;width:0.471em;"><svg xmlns="http://www.w3.org/2000/svg" width='0.471em' height='0.714em' style='width:0.471em' viewBox='0 0 471 714' preserveAspectRatio='xMinYMin'><path d='M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z'/></svg></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> as these don’t have any effect “upstream”. </li></ul><ul id="1562e7d3-01e5-80aa-b17e-e9b18601ade3" class="bulleted-list"><li style="list-style-type:disc">Q: “I am also confused because the signature for backward_pass mentions the deltas should have shapes (n, layer_size[i]), but the weight vectors have an extra bias term. This makes it hard to directly use the deltas in the update_weights function. Should the gradient wrt biases be included in deltas?”. The professor answered: “you can resolve this by simply indexing the weight matrices and applying different operations to the weights and biases” </li></ul><p id="1422e7d3-01e5-8093-a725-fc8d9ffe68ac" class="">
</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>