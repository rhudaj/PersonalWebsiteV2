<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>03 — From MapReduce to Spark</title><link rel="stylesheet" type="text/css" href="../../notion_styles.css"/></head><body><article id="10d2e7d3-01e5-80a0-83a9-d952ef13ed17" class="page sans"><header><div class="page-header-icon undefined"><img class="icon" src="https://www.notion.so/icons/folder_purple.svg"/></div><h1 class="page-title">03 — From MapReduce to Spark</h1><p class="page-description"></p></header><div class="page-body"><h3 id="1142e7d3-01e5-809e-91d7-c78ad723fbef" class="">Abstracting Hadoop MapReduce</h3><p id="1142e7d3-01e5-8016-a072-e1289ac6ef9a" class="">If we view the cluster (datacenter) as a single computer, Hadoop is the instruction set; (1) map and (2) reduce. </p><p id="1142e7d3-01e5-80b5-a8d6-d3d2f1f67c07" class="">Programming in map-reduce w’ Java is analogous to programming in assembly.</p><p id="1142e7d3-01e5-80b7-a962-fa6ce421b6b7" class="">We want something higher level (that will be translated to MapReduce). </p><figure id="1142e7d3-01e5-80f5-b9e9-c0de530d1b13" class="image"><a href="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-10-02_at_20.40.36.png"><img style="width:192px" src="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-10-02_at_20.40.36.png"/></a></figure><hr id="1142e7d3-01e5-80c0-abbb-c879e0ea0891"/><h3 id="1142e7d3-01e5-8073-aaf7-f82552f56192" class="">Implementations</h3><p id="1142e7d3-01e5-8072-b0e6-e4307c606fe8" class=""><em>Ultimately converted to MapReduce jobs</em></p><ul id="1142e7d3-01e5-80b5-8cc8-e867b31660f8" class="bulleted-list"><li style="list-style-type:disc">Hive (Facebook)</li></ul><ul id="1142e7d3-01e5-8033-92f6-c293c3185804" class="bulleted-list"><li style="list-style-type:disc">Pig (Yahoo)</li></ul><figure id="1142e7d3-01e5-8064-8e68-fad418d8d002" class="image"><a href="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-10-02_at_20.44.18.png"><img style="width:240px" src="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-10-02_at_20.44.18.png"/></a></figure><p id="1142e7d3-01e5-802f-a55d-ebffc7b2dcd2" class="">They are translate to MapReduce Jobs. But they can also talk directly with HDFS. </p><p id="1142e7d3-01e5-80e2-89eb-cb0429bca0c6" class="">Hive is implemented with SQL.</p><h3 id="1142e7d3-01e5-802c-8104-d4f3bc2c661c" class=""><strong>Pig </strong></h3><figure class="block-color-purple_background callout" style="white-space:pre-wrap;display:flex" id="1142e7d3-01e5-80f4-b71e-e795ec0d2a90"><div style="font-size:1.5em"><img class="icon" src="https://www.notion.so/icons/exclamation-mark-double_gray.svg"/></div><div style="width:100%"><p id="118d1f1b-d94b-42d3-98ac-c82278c7a51d" class=""><strong>Example — Pig </strong></p><p id="1142e7d3-01e5-80ad-bd60-d29f9ad19d77" class=""><em>Find the top 10 most visited pages in each category</em></p><hr id="1142e7d3-01e5-8017-9935-f6aca9a373b9"/><figure id="1142e7d3-01e5-80d5-aa62-e8f6c464e3c9" class="image" style="text-align:center"><a href="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-10-02_at_20.46.42.png"><img style="width:384px" src="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-10-02_at_20.46.42.png"/></a></figure><p id="1142e7d3-01e5-806b-a66f-e0bd4288150c" class="">visits = <mark class="highlight-orange">load</mark> ‘/data/visits’ as (user, url, time); </p><p id="1142e7d3-01e5-80b0-9899-f98b5459382b" class="">gVisits = <mark class="highlight-orange">group</mark> visits <mark class="highlight-orange">by</mark> url; </p><p id="1142e7d3-01e5-8059-8cf8-f2d48a7ffcfc" class="">visitCounts = <mark class="highlight-orange">foreach</mark> gVisits <mark class="highlight-orange">generate</mark> url, count(visits); </p><p id="1142e7d3-01e5-804b-b23d-e2568801bcf7" class="">urlInfo = <mark class="highlight-orange">load</mark> ‘/data/urlInfo’ <mark class="highlight-orange">as</mark> (url, category, pRank); </p><p id="1142e7d3-01e5-80ed-be41-fbb9de72d71a" class="">visitCounts = <mark class="highlight-orange">join</mark> visitCounts <mark class="highlight-orange">by</mark> url, urlInfo by url; </p><p id="1142e7d3-01e5-8063-baad-f0c4c8454a95" class="">gCategories = <mark class="highlight-orange">group</mark> visitCounts <mark class="highlight-orange">by</mark> category; </p><p id="1142e7d3-01e5-80aa-8749-f60b4748837d" class="">topUrls = <mark class="highlight-orange">foreach</mark> gCategories <mark class="highlight-orange">generate</mark> top(visitCounts,10);</p><p id="1142e7d3-01e5-80ee-8dd6-c871b30abe4f" class="">store <mark class="highlight-orange">topUrls</mark> into ‘/data/topUrls’;</p><hr id="1142e7d3-01e5-8078-b46e-d88683f34921"/><p id="1142e7d3-01e5-8062-a875-f1aa1a3279ec" class="">Query Plan: </p><figure id="1142e7d3-01e5-8021-9e24-fbec62da2f3a" class="image" style="text-align:center"><a href="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-10-02_at_20.51.11.png"><img style="width:432px" src="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-10-02_at_20.51.11.png"/></a></figure><ul id="1142e7d3-01e5-8060-b1b9-ce35f029e32d" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-orange">load</mark> file &amp; parse to text ⇒ <span style="border-bottom:0.05em solid">map</span> job </li></ul><ul id="1142e7d3-01e5-804c-a051-ca73777dd9d8" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-orange">group</mark> by ⇒ group by requires <span style="border-bottom:0.05em solid">shuffle</span> (url is key)</li></ul><ul id="1142e7d3-01e5-80ba-9e1e-f5af7906bab0" class="bulleted-list"><li style="list-style-type:disc">grouped data ⇒ reduce </li></ul></div></figure><hr id="1142e7d3-01e5-809f-a6de-f743b682cf82"/><ul id="1142e7d3-01e5-80af-aaa6-f9b097401dac" class="toggle"><li><details open=""><summary>MapReduce — Key Limitation</summary><figure id="1142e7d3-01e5-8074-a010-e2ffc5c8aa90" class="image"><a href="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-10-02_at_21.29.39.png"><img style="width:336px" src="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-10-02_at_21.29.39.png"/></a><figcaption>Lot’s of Disk/IO </figcaption></figure><figure id="1142e7d3-01e5-80e8-86cb-c16bef032d09" class="image"><a href="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-10-02_at_21.30.13.png"><img style="width:240px" src="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-10-02_at_21.30.13.png"/></a><figcaption>It’s okay not to have reduce but the output of map cannot go to another map. </figcaption></figure><p id="11a2e7d3-01e5-80d8-bdbe-e3052a413f52" class="">For a single job, you can only do 1 map &amp; 1 reduce. </p><p id="11a2e7d3-01e5-8070-be9a-edca3554c9bd" class="">⇒ if you need to do multiple transformations, you need to create <span style="border-bottom:0.05em solid">separate jobs</span></p><p id="11a2e7d3-01e5-80ab-a718-c1bfa36a9a7f" class="">⇒ <strong>Problem w’ Separate Jobs: Writing to HDFS Disk </strong><div class="indented"><p id="11a2e7d3-01e5-80a4-81ff-e9357fe70899" class="">Writing to disk is not itself the problem (i.e. there is also file access between map &amp; reduce phases). </p><p id="11a2e7d3-01e5-80cd-97bb-d4c29de59aea" class="">The problem is, between 2 jobs, the intermediate files are written to HDFS. </p><p id="11a2e7d3-01e5-80af-9a02-f3bfa998465f" class="">HDFS is <strong>slower</strong> than local disk storage <em><mark class="highlight-gray">(network transfers and block replication)</mark></em></p></div></p><p id="11a2e7d3-01e5-80c0-b506-d33a20337760" class="">⇒<strong> Bad for Iterative Algorithms </strong><em><mark class="highlight-gray">(Each iteration requires a new job)</mark></em></p></details></li></ul><ul id="1142e7d3-01e5-806b-9e06-f6c42dae1aca" class="toggle"><li><details open=""><summary>Hadoop 2 &amp; YARN</summary><p id="1142e7d3-01e5-80e2-a515-ce1dac486440" class="">Introduced YARN (Yet Another Resource Negotiator), which decouples resource management from the MapReduce framework.</p><ul id="1142e7d3-01e5-80d0-bcf2-c20b3c6c3cf6" class="bulleted-list"><li style="list-style-type:disc">Nodes are now resource managers</li></ul><ul id="1142e7d3-01e5-80b7-8960-fab7c3467d3c" class="bulleted-list"><li style="list-style-type:disc">Can do Map, Reduce &amp; more … </li></ul><ul id="1142e7d3-01e5-80ce-9c20-f216887f2e3c" class="bulleted-list"><li style="list-style-type:disc">Instead of just running map and reduce tasks, YARN allows nodes to run any task that can execute Java bytecode. This opens the door for new frameworks to run on Hadoop clusters.</li></ul></details></li></ul><hr id="11a2e7d3-01e5-80e7-b641-e677da44089f"/><h1 id="1142e7d3-01e5-8027-ab14-c9d6e46f0548" class=""><mark class="highlight-red">Apache Spark</mark></h1><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0">Overview </summary><div class="indented"><ul id="1142e7d3-01e5-80c9-bdfc-cebdfdea9446" class="bulleted-list"><li style="list-style-type:disc">Framework that builds on top of Hadoop’s YARN architecture. <ul id="1142e7d3-01e5-805a-a2e2-c16c5c6cd3f3" class="bulleted-list"><li style="list-style-type:circle">works together with Hadoop, but it on a Hadoop cluster, It can run code stand alone. </li></ul></li></ul><ul id="11a2e7d3-01e5-8040-99df-f48056e86d8f" class="bulleted-list"><li style="list-style-type:disc">Takes <code>map</code> and <code>reduce</code> and extends them across a much broader set of <em>transformations </em>&amp; <em>actions</em>. </li></ul><ul id="1142e7d3-01e5-8018-90e2-f2a755c71ed6" class="bulleted-list"><li style="list-style-type:disc">Allows: <ul id="1142e7d3-01e5-808d-960e-d719dfbf83bb" class="bulleted-list"><li style="list-style-type:circle"><span style="border-bottom:0.05em solid">in-memory computation</span>: avoids writing intermediate results to disk after every iteration/job.</li></ul><ul id="1142e7d3-01e5-8000-ac7a-eb709d87bf70" class="bulleted-list"><li style="list-style-type:circle"><span style="border-bottom:0.05em solid">flexible workflows:</span> multiple transformations without the overhead of writing to disk.</li></ul></li></ul><ul id="15b2e7d3-01e5-8019-ba64-fc91d438f96f" class="bulleted-list"><li style="list-style-type:disc"><strong>Pipeline Optimization: </strong>When you chain transformations (e.g., <code>map</code>, <code>filter</code>, <code>flatMap</code>), Spark can combine them into a single pass. </li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0">Driver, Workers</summary><div class="indented"><ul id="15b2e7d3-01e5-8076-86af-cf1a51958da4" class="bulleted-list"><li style="list-style-type:disc">they are both machines</li></ul><ul id="15b2e7d3-01e5-8073-abbe-e53676399920" class="bulleted-list"><li style="list-style-type:disc"><strong>Driver</strong><ul id="15b2e7d3-01e5-8067-8b3a-fb118a53a336" class="bulleted-list"><li style="list-style-type:circle">Central control machine </li></ul><ul id="15b2e7d3-01e5-809e-85ca-f91364827581" class="bulleted-list"><li style="list-style-type:circle">Manages the execution of tasks across the workers.</li></ul><ul id="15b2e7d3-01e5-8049-997e-e78692ed80f4" class="bulleted-list"><li style="list-style-type:circle">Aggregates results from workers. </li></ul></li></ul><ul id="15b2e7d3-01e5-80e4-ba7b-f8635aaabc68" class="bulleted-list"><li style="list-style-type:disc"><strong>Workers</strong> <ul id="15b2e7d3-01e5-8036-8f9c-d03a079bcc9d" class="bulleted-list"><li style="list-style-type:circle">Each is assigned <span style="border-bottom:0.05em solid">data partitions</span></li></ul><ul id="15b2e7d3-01e5-8013-9442-e46e582c8d39" class="bulleted-list"><li style="list-style-type:circle">process partitions</li></ul><ul id="15b2e7d3-01e5-8075-9a5c-eacce9310ccf" class="bulleted-list"><li style="list-style-type:circle">return results to the driver</li></ul></li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0">Spark vs. Hadoop</summary><div class="indented"><table id="11a2e7d3-01e5-8022-a087-fdfa0724fefa" class="simple-table"><thead class="simple-table-header"><tr id="ed900113-04f0-4b46-a466-a4f30143c741"><th id="gT~i" class="simple-table-header-color simple-table-header"></th><th id="&lt;G^w" class="simple-table-header-color simple-table-header" style="width:316.46875px">Spark</th><th id="&lt;kNi" class="simple-table-header-color simple-table-header" style="width:404.546875px">Hadoop</th></tr></thead><tbody><tr id="917b14da-3331-44a6-aa2e-8d1d2951ac4d"><th id="gT~i" class="simple-table-header-color simple-table-header">Framework Type</th><td id="&lt;G^w" class="" style="width:316.46875px">In-Memory </td><td id="&lt;kNi" class="" style="width:404.546875px">file-centric  / minimal reliance on RAM</td></tr><tr id="a7cbc2a3-86e0-41aa-8e7b-ebe971da0bb4"><th id="gT~i" class="simple-table-header-color simple-table-header">Pro</th><td id="&lt;G^w" class="" style="width:316.46875px">maximizes throughput and reduces the latency associated with disk I/O<br/>faster data processing, especially for iterative tasks.<br/></td><td id="&lt;kNi" class="" style="width:404.546875px">Even if a MapReduce worker has very little RAM available, it can still process large datasets because it streams data from files (small amount at a time).<br/>Can handle jobs where memory is constrained, making it robust even with limited system resources.<br/></td></tr><tr id="11a2e7d3-01e5-80b8-bcfe-fb2d816e244d"><th id="gT~i" class="simple-table-header-color simple-table-header">Con</th><td id="&lt;G^w" class="" style="width:316.46875px">requires more RAM. If a job exceeds the available memory, Spark’s performance may degrade or fail <em>unless properly managed.</em></td><td id="&lt;kNi" class="" style="width:404.546875px"></td></tr><tr id="11a2e7d3-01e5-80ba-8688-cfa8903a052e"><th id="gT~i" class="simple-table-header-color simple-table-header">Use when</th><td id="&lt;G^w" class="" style="width:316.46875px">Speed is critical<br/>Iterative algorithms<br/></td><td id="&lt;kNi" class="" style="width:404.546875px">• Limited memory environments<br/>• Non-iterative tasks<br/></td></tr></tbody></table><ul id="15b2e7d3-01e5-80a2-9857-df1c0239a9f4" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-default">Spark relies on </mark><mark class="highlight-default"><strong>in-memory hash tables</strong></mark><mark class="highlight-default"> for reduce-like transformations during the shuffle stage, unlike MapReduce, which uses disk-based sorting. This assumes that there is enough memory to hold these structures, </mark><mark class="highlight-default"><mark class="highlight-yellow_background">making Spark more efficient but more memory-intensive.</mark></mark></li></ul><ul id="1142e7d3-01e5-8012-aae8-eb18448a4748" class="bulleted-list"><li style="list-style-type:disc">In some cases, even when both frameworks can be used, <strong>MapReduce may still be faster</strong> because of its simplicity and the ability to process small chunks of data efficiently from large files.</li></ul><ul id="11a2e7d3-01e5-8006-b0d9-fcde58148dbc" class="bulleted-list"><li style="list-style-type:disc">Spark still has to perform <strong>shuffling </strong>— often the <strong>bottleneck </strong>— sometimes slower than writing to disk.</li></ul><ul id="11a2e7d3-01e5-80ec-994e-fce170569058" class="bulleted-list"><li style="list-style-type:disc"><strong>Shuffles </strong>only occur in <span style="border-bottom:0.05em solid">when absolutely necessary</span>, much like in MapReduce.</li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0">Spark Context</summary><div class="indented"><p id="11a2e7d3-01e5-807f-9492-cfb0c4360f9b" class="">Setting up Spark Context is simpler than MapReduce</p><ul id="1142e7d3-01e5-804e-ae30-c3a5db19c624" class="bulleted-list"><li style="list-style-type:disc"><strong>Spark Context</strong> is the main entry point for Spark functionalities, available as <code>sc</code> in the shell.</li></ul><ul id="1142e7d3-01e5-80a6-8afb-f0291632ed25" class="bulleted-list"><li style="list-style-type:disc">In standalone programs, initialize your own Spark Context with a configuration object.</li></ul></div></details><hr id="11a2e7d3-01e5-80d6-ab98-c6b6a4bfda98"/><h1 id="1142e7d3-01e5-802e-ac52-fbf349c19d56" class=""><mark class="highlight-red">RDDs</mark></h1><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0">Overview</summary><div class="indented"><p id="11a2e7d3-01e5-8091-beac-c2964de36647" class=""><strong>R</strong>esilient<strong> D</strong>istributed<strong> D</strong>ataset</p><ul id="11a2e7d3-01e5-803b-8138-e4e276aed11d" class="bulleted-list"><li style="list-style-type:disc">Abstraction for data on the cluster. </li></ul><ul id="15b2e7d3-01e5-8070-b950-db2b5c1cc61f" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-default"><code>RDD[T]</code></mark><mark class="highlight-default">: </mark>the type of data they store<ul id="15b2e7d3-01e5-8086-985e-d273884069e2" class="bulleted-list"><li style="list-style-type:circle"><code>T</code> can be anything <strong>serializable, </strong>not just KVPs</li></ul></li></ul><ul id="1142e7d3-01e5-80f4-bcad-cd50168248e5" class="bulleted-list"><li style="list-style-type:disc"><strong>Distributed</strong>: <ul id="11a2e7d3-01e5-8000-a8bb-ff12ddd1f581" class="bulleted-list"><li style="list-style-type:circle">collections of objects spread across a cluster.</li></ul><ul id="11a2e7d3-01e5-80e7-82d2-efe4177bf1b2" class="bulleted-list"><li style="list-style-type:circle">can be stored either in RAM or on disk.<ul id="11a2e7d3-01e5-802c-a0e5-c8bc3eaf21cb" class="bulleted-list"><li style="list-style-type:square">accessing data from RAM — much faster than disk.</li></ul></li></ul><ul id="11a2e7d3-01e5-805b-9409-cf3e931f65ce" class="bulleted-list"><li style="list-style-type:circle"><mark class="highlight-gray">e.g. a file with 15 HDFS blocks will create an RDD with 15 partitions</mark></li></ul></li></ul><ul id="1142e7d3-01e5-80f7-a7a5-e30fcfd28969" class="bulleted-list"><li style="list-style-type:disc"><strong>Resilient</strong>: no permanent data loss<ul id="11a2e7d3-01e5-80d3-9c06-c98a866354d6" class="bulleted-list"><li style="list-style-type:circle">Spark records the <em>lineage</em> of operations that led to the creation of each RDD. If a failure happens, it can replay these transformations on the original dataset to recover the lost data.</li></ul><ul id="11a2e7d3-01e5-807d-94af-daae1c763fe2" class="bulleted-list"><li style="list-style-type:circle">The ability to rebuild an RDD means there’s no permanent data loss, as long as the original data and lineage are available.</li></ul></li></ul><ul id="15b2e7d3-01e5-805a-bc5d-ed34474116d6" class="bulleted-list"><li style="list-style-type:disc"><strong>They can be operated on in parallel </strong><ul id="11a2e7d3-01e5-80db-b31e-d2e548b50600" class="bulleted-list"><li style="list-style-type:circle">RDD data is spread across the nodes of a cluster ⇒ parallel processing,</li></ul></li></ul><ul id="15b2e7d3-01e5-8084-bed3-cf821a1ba143" class="bulleted-list"><li style="list-style-type:disc"><strong>Optimization </strong><ul id="15b2e7d3-01e5-80a5-a2e2-f06010cdcb9e" class="bulleted-list"><li style="list-style-type:circle">Spark ensures that the partition holding the data is stored nearby the worker using it.<p id="15b2e7d3-01e5-807f-9e37-f8667f9bb6dd" class="">⇒ avoids unnecessary <strong>data shuffling</strong> across the network until required.</p></li></ul></li></ul><ul id="15b2e7d3-01e5-80d5-8312-f36011906cc2" class="bulleted-list"><li style="list-style-type:disc"><strong>Partitioning RDDs</strong><ul id="1142e7d3-01e5-802b-9a54-ee05fad61cfc" class="bulleted-list"><li style="list-style-type:circle">The default partitioning uses a <em><span style="border-bottom:0.05em solid">hash function</span></em>, calculated as: <mark class="highlight-default"><code>key % # partitions</code></mark></li></ul><ul id="1142e7d3-01e5-8043-97c4-e86df4ece84a" class="bulleted-list"><li style="list-style-type:circle">You can override the default partitioner if needed.</li></ul></li></ul><figure id="1142e7d3-01e5-800a-852b-cccd1c7c8434" class="image" style="text-align:center"><a href="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-10-02_at_22.50.56.png"><img style="width:288px" src="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-10-02_at_22.50.56.png"/></a></figure></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0"><strong>Creating RDDs</strong></summary><div class="indented"><ol type="1" id="11a2e7d3-01e5-8087-91a9-fe1eb7342b2a" class="numbered-list" start="1"><li>Parallelizing an existing collection <ul id="11a2e7d3-01e5-80ec-8952-dc9dcc32dccc" class="toggle"><li><details open=""><summary><strong>Parallelized collections</strong></summary><p id="11a2e7d3-01e5-8057-a029-c9c7b42b0112" class=""><em>sc.parallelize(data: Collection, numPartitions: int = set-automatically)</em></p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="11a2e7d3-01e5-80fb-825d-d4789fa5c5be" class="code"><code class="language-Scala">val data = Array(1,2,3,4,5) // a collection
val distData = sc.parallelize(data)  //
// distData can now be operated on in parallel </code></pre></details></li></ul></li></ol><ol type="1" id="11a2e7d3-01e5-80a1-87d6-d9d9d9caafb2" class="numbered-list" start="2"><li>Reference dataset in external filesystem (HDFS, local text-file, …)<ul id="11a2e7d3-01e5-80d5-8765-f6f8961e8f43" class="toggle"><li><details open=""><summary><strong>External Files</strong></summary><p id="11a2e7d3-01e5-8065-9f17-f610dc551864" class=""><em>sc.textFile(path: String, numPartitions: int = auto-set)</em></p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="11a2e7d3-01e5-8025-be61-f2fc1198b446" class="code"><code class="language-Scala">val distFile: RDD[String] = sc.textFile(&quot;data.txt&quot;)</code></pre><p id="11a2e7d3-01e5-8066-bdd6-d4894e64e559" class="">&quot;*/.txt&quot; loads multiple files, concats them together.</p></details></li></ul></li></ol></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0"><strong>Persist / Cache</strong></summary><div class="indented"><p id="11a2e7d3-01e5-8080-8ae2-ee7fd6a17b7a" class="">By default, each transformed RDD needs to be recomputed each time you run an action on it. </p><p id="11a2e7d3-01e5-8063-b621-f013f05c64cf" class=""><code>&lt;RDD&gt;.persist()</code>/ <code>&lt;RDD&gt;.cache()</code> </p><p id="11a2e7d3-01e5-80f9-be97-ff280b299ed4" class=""><strong>what? </strong></p><ul id="15b2e7d3-01e5-80f6-a9cc-eeb7e0bb5247" class="bulleted-list"><li style="list-style-type:disc">Keep the RDD around in the cluster for faster access net time you run an action on it. </li></ul><p id="15b2e7d3-01e5-80a3-bffb-eff8d5fbbec5" class=""><strong>why? </strong></p><ul id="15b2e7d3-01e5-805d-aae4-fabbc6401293" class="bulleted-list"><li style="list-style-type:disc">Improvees performance by keeping the data in memory, allowing subsequent actions to access it quickly without re-computing prior transformations</li></ul><p id="15b2e7d3-01e5-8075-b963-d5f1c91f65c6" class=""><strong>When? </strong></p><ul id="15b2e7d3-01e5-8033-921d-cb1530fa5bdf" class="bulleted-list"><li style="list-style-type:disc">Use it when you need to use the RDD values many times. </li></ul><ul id="15b2e7d3-01e5-8002-9511-f684e0831887" class="bulleted-list"><li style="list-style-type:disc">Otherwise your adding overhead for no reason</li></ul><ul id="15b2e7d3-01e5-8079-97eb-fd91b6cea5c6" class="bulleted-list"><li style="list-style-type:disc">driver has a finite amount of memory available.</li></ul><p id="15b2e7d3-01e5-8012-be9c-f44642d6e831" class=""><strong>Trade-Off</strong>:</p><ul id="2690ab46-e8a3-4cc0-9c67-84160663381c" class="bulleted-list"><li style="list-style-type:disc">caching for speed ↔ performance penalties from increased memory usage</li></ul><p id="15b2e7d3-01e5-80bd-82db-ec5a1a71831b" class=""><strong>Different persistence levels:</strong></p><ul id="15b2e7d3-01e5-80a5-86c0-f259f125cada" class="bulleted-list"><li style="list-style-type:disc">in memory, on disk, replicate across nodes, etc.</li></ul><ul id="15b2e7d3-01e5-80b6-a73d-f5d826aba8d3" class="bulleted-list"><li style="list-style-type:disc">If Spark doesn&#x27;t have enough memory to fully cache an RDD in RAM, it will spill to disk.</li></ul><ul id="15b2e7d3-01e5-805f-875b-d30f861591f1" class="toggle"><li><details open=""><summary><mark class="highlight-purple"><strong>Example</strong></mark></summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="11a2e7d3-01e5-80a5-80d2-f8336d6ec00d" class="code"><code class="language-Scala">val lines = sc.textFile(&quot;data.txt&quot;)
val lineLengths = lines.map(s =&gt; s.length) // transform
val totalLength = lineLengths.reduce((a,b)=&gt;a+b) // action

lineLengths.persist() // persist for future actions
...</code></pre></details></li></ul><ul id="1142e7d3-01e5-8049-907d-ffcf785dcfa1" class="toggle"><li><details open=""><summary><strong>Caching Improves Fault Tolerance</strong></summary><ul id="15b2e7d3-01e5-808a-9bae-ed81524bcf18" class="bulleted-list"><li style="list-style-type:disc"><strong>Intermediate Files</strong>:<ul id="1142e7d3-01e5-805e-bd29-e17ef229a8ae" class="bulleted-list"><li style="list-style-type:circle">Spark keeps intermediate files during shuffle operations, which can be helpful for recovery. Even without caching, Spark often retrieves data from the most recent shuffle if needed.</li></ul><ul id="1142e7d3-01e5-8095-8d69-fb2ba59aa752" class="bulleted-list"><li style="list-style-type:circle">However, if a worker fails before the shuffle is completed, those intermediate files may not be available for recovery, leading to potential data loss or the need for recomputation.</li></ul></li></ul><ul id="15b2e7d3-01e5-80f3-b708-e4072efb1d5c" class="bulleted-list"><li style="list-style-type:disc">caching can improve fault tolerance by enabling quicker recovery from failures, especially for expensive operations. However, it also increases memory usage, which must be balanced against the overall performance and efficiency of the Spark application.</li></ul></details></li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0">Fault Recovery</summary><div class="indented"><p id="1142e7d3-01e5-8079-aab0-e1192fa0b708" class="">RDDs track lineage information that can be used to efficiently recompute lost data</p><figure id="1142e7d3-01e5-8092-8f79-e3efce1d3c0f" class="image"><a href="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-10-02_at_23.09.28.png"><img style="width:480px" src="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-10-02_at_23.09.28.png"/></a></figure><ul id="1142e7d3-01e5-800a-9067-cf0aa598fd52" class="bulleted-list"><li style="list-style-type:disc">if i have rdd and 10 workers hold data, you&#x27;ve lost that partition, need to recreate it.<br/>• Driver can do it. It knows all. Gos back to input file and recreates missing.<br/>• But if theres a shuffle, needs to recreate all of them from scratch (SLOW)<br/>• But, if you persisted the part after most intensive part to disk, you can recover it faster (without rewinding to beginning)<br/></li></ul><ul id="1142e7d3-01e5-8004-bc82-d44613cee833" class="bulleted-list"><li style="list-style-type:disc">so 2nd reason to use cache/persists<br/>• so that if the system does a fault recovery, it will be quicker.<br/>• so if you anticipate such a fault.<br/></li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0">Broadcast </summary><div class="indented"><ul id="15b2e7d3-01e5-80ea-b71f-fe7b08eca99f" class="bulleted-list"><li style="list-style-type:disc">If you Broadcast a value, each worker machine gets one copy.</li></ul><ul id="15b2e7d3-01e5-80ca-a19b-fdd1377fe735" class="bulleted-list"><li style="list-style-type:disc">Broadcast variables are read-only</li></ul></div></details><h1 id="11a2e7d3-01e5-8077-9755-e34a469bad05" class=""><mark class="highlight-red">RDD Operations</mark></h1><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0">Overview</summary><div class="indented"><p id="15b2e7d3-01e5-800c-ba8c-d66e7bf47d62" class="">Methods applied to <mark class="highlight-default"><code>RDD[T]</code></mark></p><ol type="1" id="11a2e7d3-01e5-8090-82d1-f323ac64e676" class="numbered-list" start="1"><li><strong>Transformations</strong>:<ul id="15b2e7d3-01e5-80e1-b89f-d4445f8ca98b" class="bulleted-list"><li style="list-style-type:disc">They define what to do but don&#x27;t execute until an action is triggered (<span style="border-bottom:0.05em solid">lazy</span>)</li></ul><ul id="11a2e7d3-01e5-809f-ba65-ec9c6f6a84bd" class="bulleted-list"><li style="list-style-type:disc"><strong>why? </strong>only need to output results that are needed, not intermediate results. </li></ul><ul id="15b2e7d3-01e5-80f7-8531-cf787258ca85" class="toggle"><li><details open=""><summary><strong>Narrow vs. Wide Dependency </strong></summary><table id="9a4481c5-2f4b-4da6-8ff5-9ec9da55ded5" class="simple-table"><thead class="simple-table-header"><tr id="47eb971a-7b86-482a-8466-d71121522f2d"><th id="hVVn" class="simple-table-header-color simple-table-header">Dependency</th><th id="c@Y}" class="simple-table-header-color simple-table-header">Operation Type</th><th id="OS_=" class="simple-table-header-color simple-table-header"># Partitions</th><th id="F_L&gt;" class="simple-table-header-color simple-table-header">can set level of parallelism</th><th id="B[_[" class="simple-table-header-color simple-table-header">speed</th></tr></thead><tbody><tr id="461eb83a-125d-4eba-a6d6-ccba216c9df1"><td id="hVVn" class="">Narrow</td><td id="c@Y}" class="">map-like </td><td id="OS_=" class=""># partitons in source RDD == # partitions in destination RDD</td><td id="F_L&gt;" class="">❌<br/>Changing # of partitions during a narrow dependency would require a shuffle<br/></td><td id="B[_[" class="">efficient in-memory processing without network transfers.</td></tr><tr id="4dd372d1-2a0d-442d-9cba-863cfbe9b29c"><td id="hVVn" class="">Wide</td><td id="c@Y}" class="">reduce-like</td><td id="OS_=" class="">Requires shuffling as the partitions are not directly aligned ⇒ # partitions can change</td><td id="F_L&gt;" class="">✅<br/>shuffling data ⇒ can change # of partitions<br/></td><td id="B[_[" class="">Slow</td></tr></tbody></table><ul id="4b6a953a-5147-4ab7-b69b-df64661de34d" class="bulleted-list"><li style="list-style-type:disc">e.g. if you have an RDD with 5 partitions and you apply a <code>map</code>, the resulting RDD also has 5 partitions. </li></ul><ul id="059516f8-8911-49ae-a4e7-673ee2a26cb6" class="toggle"><li><details open=""><summary><mark class="highlight-purple"><strong>Example — Word Count</strong></mark></summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="87cab41a-1ec8-46b0-b0c7-96d9852af402" class="code"><code class="language-Scala">val textFile = sc.textFile(“hamlet.txt”)
textFile
	.flatMap(line =&gt; line.split(“ “)) // tokenize
	.map(word =&gt; (word, 1)) // map-like (narrow)
	.reduceByKey((x, y) =&gt; x + y) //reduce-like (wide)
	.saveAsTextFile(“results”) // action</code></pre><figure id="4086c7e7-6a45-4f69-8b51-44efe099f492" class="image" style="text-align:center"><a href="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-10-02_at_23.28.40.png"><img style="width:384px" src="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-10-02_at_23.28.40.png"/></a><figcaption>2 <strong>partitions</strong> </figcaption></figure><ul id="d27d838e-f285-4020-975c-7c9c81569682" class="bulleted-list"><li style="list-style-type:disc">map-like — <strong>narrow dependency</strong><ul id="00d6fc78-1669-4533-a318-67adde5f2fd6" class="bulleted-list"><li style="list-style-type:circle">(to,1) depended only on 1st partition</li></ul></li></ul><ul id="e1bf8193-81bf-4a77-9409-0d0a7c1e8367" class="bulleted-list"><li style="list-style-type:disc">reduceBykey — wide dependency<ul id="b2aefa33-c29a-44f4-9ec9-3b9c6acaaa90" class="bulleted-list"><li style="list-style-type:circle">(be,2) depends on previous RDD in pipeline</li></ul></li></ul></details></li></ul></details></li></ul><ul id="15b2e7d3-01e5-80dd-9fe5-db3e1bb23df1" class="toggle"><li><details open=""><summary><strong>Setting Level of Parallelism </strong></summary><p id="6618a3ed-4313-4c5b-a217-6eda616c9a6a" class="">All the KVP RDD operations take an <strong>optional second parameter</strong> for # of tasks: </p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="96636eb9-6281-4224-ac15-3b0c13d2a5ac" class="code"><code class="language-Scala">words.reduceByKey(lambda x, y: x + y, 5 )
words.groupByKey( 5 )
visits.join(pageViews, 5 )</code></pre><ul id="018514f1-fa8f-4ed8-944e-c53aeb5e9ab9" class="bulleted-list"><li style="list-style-type:disc"><strong>default number of partitions:</strong> <ul id="4392ee8e-c479-47d9-856c-867eb1fe3ba3" class="bulleted-list"><li style="list-style-type:circle">in a Spark job is often set to the # of logical cores available on the machine</li></ul></li></ul><ul id="2614f0eb-7a10-4ae7-9497-c3be4a1207f7" class="bulleted-list"><li style="list-style-type:disc">Reduce # of partitions ⇒ improve performance <ul id="b8e7c840-136a-43f7-92c4-0542cf7693b9" class="bulleted-list"><li style="list-style-type:circle">e.g. if an RDD starts with many partitions due to a large input dataset, you may know that the resulting data will be significantly smaller, allowing for a decrease in partitions (e.g., from 20 to 5)</li></ul></li></ul><ul id="52c9b0f1-3192-4d9d-9019-b048b02f5afd" class="bulleted-list"><li style="list-style-type:disc">For joins: <ul id="9ffa8632-81cd-4e35-8f8c-4f5e682e077e" class="bulleted-list"><li style="list-style-type:circle">defaults to the larger # of partitions between the two RDDs being joined.</li></ul><ul id="64d29d3a-6cd4-47ba-b9c2-6eb6cc639973" class="bulleted-list"><li style="list-style-type:circle">More is better since joins can lead to an increase in the number of resulting rows.</li></ul></li></ul></details></li></ul></li></ol><ol type="1" id="11a2e7d3-01e5-807d-886f-c721a04bbb7d" class="numbered-list" start="2"><li><strong>Actions</strong>:<ul id="15b2e7d3-01e5-80af-b075-d1904765dafa" class="bulleted-list"><li style="list-style-type:disc"><strong>what? </strong><p id="15b2e7d3-01e5-80cc-bdd1-e72160339465" class="">Actions materialize the RDD by returning values to the driver program or HDFS</p></li></ul><ul id="15b2e7d3-01e5-80c3-8e81-e12b514f9687" class="bulleted-list"><li style="list-style-type:disc"><strong>how? </strong><p id="15b2e7d3-01e5-802c-b24d-ef3aff6ba273" class="">When an action (e.g. <code>count()</code>) is called, it forces the retrieval of values from the initial source (HDFS) through all the transformations leading back to the base RDD (in the driver program)</p></li></ul></li></ol><ol type="1" id="11a2e7d3-01e5-80fa-aa9c-e444cf87ce13" class="numbered-list" start="3"><li><strong>Utility Methods</strong>:<ul id="15b2e7d3-01e5-80d4-a397-cf9a29a97716" class="bulleted-list"><li style="list-style-type:disc">e.g. <code>cache()</code>, <code>persisit()</code></li></ul></li></ol></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0"><mark class="highlight-orange"><strong>Transformations</strong></mark></summary><div class="indented"><h3 id="11a2e7d3-01e5-8003-ac65-de25bb017e5e" class=""><mark class="highlight-orange"><strong>”Map-Like”</strong></mark></h3><table id="11a2e7d3-01e5-8018-96b9-daba5c826c1c" class="simple-table"><thead class="simple-table-header"><tr id="adf7092b-7089-4167-8d7a-d33933fe0763"><th id="@Kt^" class="simple-table-header-color simple-table-header" style="width:280px">RDD Transformation</th><th id="h]:[" class="simple-table-header-color simple-table-header" style="width:153px">Params</th><th id="Y=^t" class="simple-table-header-color simple-table-header" style="width:254px">What</th><th id="~rrS" class="simple-table-header-color simple-table-header" style="width:221px">Returns</th></tr></thead><tbody><tr id="b3074000-a45b-4bc6-bac0-3ffcc042a37f"><td id="@Kt^" class="" style="width:280px"><mark class="highlight-default"><code>.map</code></mark></td><td id="h]:[" class="" style="width:153px">f: (T) ⇒ U</td><td id="Y=^t" class="" style="width:254px">Applies the function <code>f</code> to each element of the source RDD</td><td id="~rrS" class="" style="width:221px">new RDD of type <code>U</code></td></tr><tr id="a66bb595-9784-4b0b-8ef2-c62d6721f9fa"><td id="@Kt^" class="" style="width:280px"><mark class="highlight-default"><code>.filter</code></mark></td><td id="h]:[" class="" style="width:153px">f: (T) =&gt; Boolean</td><td id="Y=^t" class="" style="width:254px">Returns a new RDD containing only the elements from the source RDD for which the function returns true.</td><td id="~rrS" class="" style="width:221px">new RDD of type <code>T</code></td></tr><tr id="11a2e7d3-01e5-80af-b996-caacb356a575"><td id="@Kt^" class="" style="width:280px"><mark class="highlight-default"><mark class="highlight-default_background"><code>.flatMap</code></mark></mark></td><td id="h]:[" class="" style="width:153px">f: (T) =&gt; TraversableOnce[U]</td><td id="Y=^t" class="" style="width:254px">Map, but each input item can be mapped to 0+ output items.</td><td id="~rrS" class="" style="width:221px">new RDD of type <code>U</code></td></tr><tr id="11a2e7d3-01e5-805b-9c80-d59f0a03806a"><td id="@Kt^" class="" style="width:280px"><mark class="highlight-default"><mark class="highlight-default_background"><code>.mapPartitions</code></mark></mark></td><td id="h]:[" class="" style="width:153px">f: (Iterator[T]) =&gt; Iterator[U]</td><td id="Y=^t" class="" style="width:254px">Similar to <mark class="highlight-gray"><code>map</code></mark>, but runs separately for each partition (block) of the RDD</td><td id="~rrS" class="" style="width:221px"></td></tr><tr id="11a2e7d3-01e5-8008-83d7-e494a5db152c"><td id="@Kt^" class="" style="width:280px"><mark class="highlight-default"><mark class="highlight-default_background"><code>.repartitionAndSortWithinPartitions</code></mark></mark></td><td id="h]:[" class="" style="width:153px">partitioner</td><td id="Y=^t" class="" style="width:254px">Repartition the RDD according to the partitioner, sort by keys within each partition. </td><td id="~rrS" class="" style="width:221px"></td></tr></tbody></table><h3 id="1142e7d3-01e5-80d1-ba9d-e815ba5598f1" class=""><mark class="highlight-orange"><strong>“Reduce-like” </strong></mark></h3><table id="11a2e7d3-01e5-804f-8c93-f3cbaf5fbea3" class="simple-table"><thead class="simple-table-header"><tr id="06de1659-d7ee-4b2d-87ad-12b93d77c030"><th id="@Kt^" class="simple-table-header-color simple-table-header" style="width:138.99815368652344px">RDD Transformation</th><th id="Lyqi" class="simple-table-header-color simple-table-header" style="width:61.99999237060547px">Takes</th><th id="h]:[" class="simple-table-header-color simple-table-header" style="width:171.9816131591797px">Params (all have (opt) numPartitons)</th><th id="~rrS" class="simple-table-header-color simple-table-header" style="width:77.98161315917969px">Returns</th><th id="Y=^t" class="simple-table-header-color simple-table-header" style="width:254px">What</th><th id="Hs^c" class="simple-table-header-color simple-table-header" style="width:249px">Efficiency</th></tr></thead><tbody><tr id="4aa7dd36-4726-44c5-b6ac-cbe87737831b"><td id="@Kt^" class="" style="width:138.99815368652344px"><mark class="highlight-default"><code>.groupByKey</code></mark></td><td id="Lyqi" class="" style="width:61.99999237060547px"></td><td id="h]:[" class="" style="width:171.9816131591797px"></td><td id="~rrS" class="" style="width:77.98161315917969px">(k, V[])</td><td id="Y=^t" class="" style="width:254px">brings all key-value pairs with the same key to a single place, but it does <em>not</em> perform any reduction on these values.</td><td id="Hs^c" class="" style="width:249px">can be inefficient; all values are shuffled across the network; potent high memory usage.</td></tr><tr id="4dbb4294-6e74-4e33-8fc0-f3c7415b0c49"><td id="@Kt^" class="" style="width:138.99815368652344px"><mark class="highlight-default"><code>.reduceByKey</code></mark></td><td id="Lyqi" class="" style="width:61.99999237060547px">(K,V)</td><td id="h]:[" class="" style="width:171.9816131591797px">f: (V,V) ⇒ V<br/><br/></td><td id="~rrS" class="" style="width:77.98161315917969px">(K,V)[]</td><td id="Y=^t" class="" style="width:254px">Combines values within partitions and then shuffles reduced values.</td><td id="Hs^c" class="" style="width:249px">Since combining occurs locally within partitions before the shuffle, <code>reduceByKey</code> is generally more efficient than <code>groupByKey</code>.</td></tr><tr id="92f8cecf-d616-41e3-b669-84db5650484b"><td id="@Kt^" class="" style="width:138.99815368652344px"><mark class="highlight-default"><code>.aggregateByKey</code></mark></td><td id="Lyqi" class="" style="width:61.99999237060547px">(K,V)</td><td id="h]:[" class="" style="width:171.9816131591797px">1. init<br/>2. append<br/>3. merge<br/></td><td id="~rrS" class="" style="width:77.98161315917969px">(k, U)</td><td id="Y=^t" class="" style="width:254px">More flexible:<br/>1. init: initial value [type C]<br/>2. append: take a C and add V<br/>3. merge: combine two C <br/></td><td id="Hs^c" class="" style="width:249px">benefits from local aggregations within partitions before the shuffle.</td></tr><tr id="9a23d543-3994-4ab4-ab89-f9b70d2186c3"><td id="@Kt^" class="" style="width:138.99815368652344px"><mark class="highlight-default"><code>.combineByKey</code></mark></td><td id="Lyqi" class="" style="width:61.99999237060547px">(K,V)</td><td id="h]:[" class="" style="width:171.9816131591797px">1. create <br/>2. append<br/>3. merge<br/></td><td id="~rrS" class="" style="width:77.98161315917969px">(K,C)</td><td id="Y=^t" class="" style="width:254px">Most flexible:<br/>1. create: make a C from a V <br/>2. append: take a C and add V to it<br/>3. combine two C<br/></td><td id="Hs^c" class="" style="width:249px"></td></tr><tr id="15a2e7d3-01e5-80b9-b30a-e5f5ff3153f8"><td id="@Kt^" class="" style="width:138.99815368652344px"><mark class="highlight-default"><code>.sortByKey</code></mark></td><td id="Lyqi" class="" style="width:61.99999237060547px"></td><td id="h]:[" class="" style="width:171.9816131591797px"></td><td id="~rrS" class="" style="width:77.98161315917969px"></td><td id="Y=^t" class="" style="width:254px">requires a shuffle</td><td id="Hs^c" class="" style="width:249px"></td></tr><tr id="15b2e7d3-01e5-803e-bcfb-f032d6e2b648"><td id="@Kt^" class="" style="width:138.99815368652344px"><mark class="highlight-default"><mark class="highlight-default_background"><code>join()</code></mark></mark></td><td id="Lyqi" class="" style="width:61.99999237060547px"></td><td id="h]:[" class="" style="width:171.9816131591797px"></td><td id="~rrS" class="" style="width:77.98161315917969px"></td><td id="Y=^t" class="" style="width:254px">inner join, by key</td><td id="Hs^c" class="" style="width:249px"></td></tr><tr id="15b2e7d3-01e5-80db-b63a-c4e85be69940"><td id="@Kt^" class="" style="width:138.99815368652344px"><mark class="highlight-default_background"><code>cogroup()</code></mark></td><td id="Lyqi" class="" style="width:61.99999237060547px"></td><td id="h]:[" class="" style="width:171.9816131591797px"></td><td id="~rrS" class="" style="width:77.98161315917969px"></td><td id="Y=^t" class="" style="width:254px">join, but keeps the values in an iterator, instead of seperate pairs</td><td id="Hs^c" class="" style="width:249px"></td></tr></tbody></table><p id="11a2e7d3-01e5-80dc-81ba-e5b80856e806" class="">Always use simplest function to get job done, in order of most efficient:</p><ol type="1" id="11a2e7d3-01e5-8066-a05b-d830971e02f4" class="numbered-list" start="1"><li>reduceByKey</li></ol><ol type="1" id="11a2e7d3-01e5-803d-a1e1-d5b9a7f2567d" class="numbered-list" start="2"><li>combineByKey<ul id="15b2e7d3-01e5-8008-9acd-edb782bfdde0" class="bulleted-list"><li style="list-style-type:disc">more fine grained</li></ul><ul id="15b2e7d3-01e5-80a5-9bf6-d77b2c914b51" class="bulleted-list"><li style="list-style-type:disc"><code>reduceByKey(reduce)</code> calls <code>combineByKey(identity, reduce, reduce)</code></li></ul></li></ol><ol type="1" id="15b2e7d3-01e5-80d7-a579-f6b3358c20a0" class="numbered-list" start="3"><li>Aggregate By Key <ul id="15b2e7d3-01e5-80c1-a92e-f23aa55078e6" class="bulleted-list"><li style="list-style-type:disc">between reduceByKey and combineByKey</li></ul></li></ol><ol type="1" id="11a2e7d3-01e5-8068-a2d4-fce40b710c55" class="numbered-list" start="4"><li>groupByKey</li></ol><p id="11a2e7d3-01e5-8015-9b27-ce28fb674f2c" class="">Detailed Explanations</p><ul id="1142e7d3-01e5-80c4-a274-ea3747469421" class="toggle"><li><details open=""><summary>groupByKey</summary><ul id="1142e7d3-01e5-80df-af24-e9d61ef9a716" class="bulleted-list"><li style="list-style-type:disc">For each <code>(K, V)</code> pair in the RDD:<ul id="1142e7d3-01e5-8085-ac73-fba54ff2bbd2" class="bulleted-list"><li style="list-style-type:circle">If <code>K</code> is not in the hash table (HT), create an entry for <code>K</code> with the value <code>V</code>.</li></ul><ul id="1142e7d3-01e5-80c5-af0f-e628085c4db7" class="bulleted-list"><li style="list-style-type:circle">If <code>K</code> is already in HT, append <code>V</code> to the list of values associated with <code>K</code>.</li></ul></li></ul><ul id="1142e7d3-01e5-8074-bce5-db8ce3967137" class="bulleted-list"><li style="list-style-type:disc">After the shuffle, the values are grouped by key, and you can later apply transformations like <code>map</code>, <code>flatMap</code>, etc., to process them further.</li></ul><ul id="1142e7d3-01e5-80e9-95a0-d2ebe7c0d977" class="bulleted-list"><li style="list-style-type:disc"><strong>Efficiency</strong>: It can be inefficient because all values are shuffled across the network, potentially leading to high memory usage.</li></ul></details></li></ul><ul id="1142e7d3-01e5-8004-ab03-f71a822af771" class="toggle"><li><details open=""><summary>reduceByKey</summary><ol type="1" id="1142e7d3-01e5-80c1-a0cc-ccca07b0cc72" class="numbered-list" start="1"><li><strong>Combining within a partition</strong>:<ul id="1142e7d3-01e5-80d3-8d53-dcc2d518cf91" class="bulleted-list"><li style="list-style-type:disc">For each <code>(K, V)</code> pair in the RDD:<ul id="1142e7d3-01e5-8036-b5d3-d98f249bb9e3" class="bulleted-list"><li style="list-style-type:circle">If <code>K</code> is not in HT, associate <code>K</code> with <code>V</code> in HT.</li></ul><ul id="1142e7d3-01e5-801e-ae02-dc0c85449af7" class="bulleted-list"><li style="list-style-type:circle">If <code>K</code> already exists in HT, replace the old value <code>V_old</code> with <code>f(V_old, V)</code>, where <code>f</code> is the function provided to combine the values.</li></ul></li></ul></li></ol><ol type="1" id="1142e7d3-01e5-8008-98c3-e0e82f03c32e" class="numbered-list" start="2"><li><strong>Shuffle</strong>: Each worker will receive key-value pairs for specific keys.</li></ol><ol type="1" id="1142e7d3-01e5-8040-9189-e7356f67d04d" class="numbered-list" start="3"><li><strong>Combining across partitions</strong>:<ul id="1142e7d3-01e5-8068-ab50-f2e5d81a91e1" class="bulleted-list"><li style="list-style-type:disc">Once a worker receives all key-value pairs for a specific key, it repeats step 1 using the same <code>f</code> function to merge the results.</li></ul></li></ol><ul id="1142e7d3-01e5-80d2-b6f4-c76a188fc9c5" class="bulleted-list"><li style="list-style-type:disc"><strong>In-Memory Hash Tables</strong>: Spark uses in-memory hash tables instead of sorting the values, as MapReduce does, assuming there is enough RAM for processing.</li></ul><ul id="1142e7d3-01e5-8033-bee8-eb9f02af3f9b" class="bulleted-list"><li style="list-style-type:disc"><strong>Example</strong>: In a word count, you would apply <code>reduceByKey(lambda x, y: x + y)</code> to sum up the counts of each word.</li></ul><ul id="11a2e7d3-01e5-80c4-9108-ffde9aede3e1" class="bulleted-list"><li style="list-style-type:disc"><strong>reduceByKey(_ + _)</strong><ul id="11a2e7d3-01e5-80ca-90e2-fbce0879ac47" class="bulleted-list"><li style="list-style-type:circle">combine all values for each unique key.</li></ul></li></ul></details></li></ul><ul id="1142e7d3-01e5-800d-af8c-f677718bbe98" class="toggle"><li><details open=""><summary>aggregateByKey</summary><ol type="1" id="1142e7d3-01e5-80f8-a788-e1035f3ad992" class="numbered-list" start="1"><li><strong>Within partitions</strong>:<ul id="1142e7d3-01e5-800e-9893-e0d085e5306d" class="bulleted-list"><li style="list-style-type:disc">For each <code>(K, V)</code> pair:<ul id="1142e7d3-01e5-80dc-87dd-c5c300f53b5d" class="bulleted-list"><li style="list-style-type:circle">If <code>K</code> is not in HT, insert <code>zero</code> into HT for <code>K</code>, and apply the <code>insert</code> function (<code>insert(zero, V)</code>).</li></ul><ul id="1142e7d3-01e5-80ca-9999-e6c19393a19f" class="bulleted-list"><li style="list-style-type:circle">If <code>K</code> is already in HT, retrieve the accumulator <code>U</code> and update it using <code>insert(U, V)</code>.</li></ul></li></ul></li></ol><ol type="1" id="1142e7d3-01e5-809d-a1be-d6ae60f7f3ee" class="numbered-list" start="2"><li><strong>Shuffle</strong>: Each worker receives key-value pairs for specific keys.</li></ol><ol type="1" id="1142e7d3-01e5-8068-a981-c99b4e7483ed" class="numbered-list" start="3"><li><strong>Across partitions</strong>:<ul id="1142e7d3-01e5-80b9-87e8-d1267a27a6d2" class="bulleted-list"><li style="list-style-type:disc">Once a worker receives all key-value pairs for a specific key, it uses the <code>merge</code> function to combine the accumulators from different partitions.</li></ul></li></ol><ul id="1142e7d3-01e5-80d4-b411-c8b0f2d0f74e" class="bulleted-list"><li style="list-style-type:disc"><strong>Example</strong>: If you&#x27;re computing an average, you could use <code>aggregateByKey</code> with a zero value like <code>(0, 0)</code> (count and sum), an insert function to increment both, and a merge function to combine the accumulators across partitions.</li></ul></details></li></ul><ul id="1142e7d3-01e5-8009-86d3-fb40c9a6a7fb" class="toggle"><li><details open=""><summary>combineByKey</summary><ol type="1" id="1142e7d3-01e5-809f-adc7-f37f715fd4be" class="numbered-list" start="1"><li>For each <code>(K, V)</code> pair:<ul id="1142e7d3-01e5-8006-a117-f1cfe30892ee" class="bulleted-list"><li style="list-style-type:disc">If <code>K</code> is not in HT, the first value is turned into an accumulator using a user-provided function (instead of starting from a <code>zero</code>).</li></ul></li></ol><ol type="1" id="1142e7d3-01e5-8078-b754-d2fbfef216d2" class="numbered-list" start="2"><li>The same shuffle and across-partition merge steps as in <code>aggregateByKey</code> occur.</li></ol><ul id="1142e7d3-01e5-8093-a494-d6c8cd0505e6" class="bulleted-list"><li style="list-style-type:disc"><strong>Example</strong>: <code>reduceByKey(f)</code> is essentially implemented as <code>combineByKey(identity, f, f)</code>, where <code>identity</code> turns the first value into the accumulator.</li></ul></details></li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0"><mark class="highlight-orange">Actions</mark></summary><div class="indented"><p id="15b2e7d3-01e5-8055-a3fa-e9dd807869f5" class=""><code>coalesce</code></p><ul id="15b2e7d3-01e5-8071-a7a8-cb761bc59c5c" class="bulleted-list"><li style="list-style-type:disc">Can be used to only reduce  the number of partitions. </li></ul><ul id="15b2e7d3-01e5-80fa-9da5-ed6a1e966d93" class="bulleted-list"><li style="list-style-type:disc">It avoids full shuffling so it is faster than repartition but it may give unbalanced partitions.</li></ul><p id="15b2e7d3-01e5-8084-b94c-cac2de96c8cb" class=""><code>repartition</code></p><ul id="15b2e7d3-01e5-8023-b333-d69de9e33d40" class="bulleted-list"><li style="list-style-type:disc">triggers shuffling but it gives more balanced partitions. </li></ul><ul id="15b2e7d3-01e5-802c-973b-c17596e3cfbe" class="bulleted-list"><li style="list-style-type:disc">Can be used to increase or decrease the number of partitions. </li></ul><p id="11a2e7d3-01e5-80b4-987d-ca89a3e6b6e5" class=""><code>collect()</code></p><ul id="11a2e7d3-01e5-805f-a8c0-f098f0bcdec0" class="bulleted-list"><li style="list-style-type:disc">dont use unless sure your not collecting huge amount of data (to not overload driver)</li></ul><ul id="15b2e7d3-01e5-8088-9f79-fc1069c9a2d4" class="bulleted-list"><li style="list-style-type:disc">Gathers results <span style="border-bottom:0.05em solid">across partitions</span>.</li></ul><p id="11a2e7d3-01e5-801a-b196-ecd16583e72c" class=""><code>take(n)</code></p><ul id="15b2e7d3-01e5-80f3-98c7-e2a459377945" class="bulleted-list"><li style="list-style-type:disc">returns first <code>n</code></li></ul><ul id="11a2e7d3-01e5-8003-ad8b-d9b0e5b0d2ce" class="bulleted-list"><li style="list-style-type:disc">useful for testing</li></ul><p id="11a2e7d3-01e5-80ed-b58d-fa2563c1242e" class=""><code>count()</code></p><ul id="11a2e7d3-01e5-80ee-a56c-d1faa081378c" class="bulleted-list"><li style="list-style-type:disc">how many there are</li></ul><p id="11a2e7d3-01e5-80d1-aa53-ecddc3ac6741" class=""><code>reduce()</code></p><ul id="15b2e7d3-01e5-803c-bd10-c1ac4a36fccd" class="bulleted-list"><li style="list-style-type:disc"><strong>what? </strong><p id="11a2e7d3-01e5-8062-8583-f44b23eb3923" class="">reduces entire RDD into a singe value using a function </p></li></ul><ul id="15b2e7d3-01e5-8049-8c3e-f68fef03c9af" class="bulleted-list"><li style="list-style-type:disc"><strong>how? </strong><ul id="15b2e7d3-01e5-80cc-9895-d9c7f7987377" class="bulleted-list"><li style="list-style-type:circle">KVP are partitioned and shuffled by Partitioner</li></ul><ul id="15b2e7d3-01e5-80dc-9f7d-fbbbfc0f2ac6" class="bulleted-list"><li style="list-style-type:circle">calls reduce on keys <strong>in sorted order</strong></li></ul></li></ul><p id="11a2e7d3-01e5-80a4-a0e8-e685cdcb668a" class=""><code>sum()</code></p><ul id="11a2e7d3-01e5-804b-aa99-e96c110ae8af" class="bulleted-list"><li style="list-style-type:disc">adds them all up (for RDDs of int, float, etc.)</li></ul><p id="11a2e7d3-01e5-80f8-ba9a-e89de3e3ff4b" class=""><code>saveAsTextFile()</code></p><ul id="11a2e7d3-01e5-80b6-bfa7-cd59a3ac1df8" class="bulleted-list"><li style="list-style-type:disc">does not actualy save as a text file. It&#x27;s a directory with the name &quot;file.txt&quot; containing partition files.</li></ul><ul id="11a2e7d3-01e5-807e-915b-cc093bf0d7d7" class="bulleted-list"><li style="list-style-type:disc">Some of these files may be empty.</li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0">Reduce vs. ReduceByKey</summary><div class="indented"><p id="15b2e7d3-01e5-80f3-9947-edf9b38360ff" class="">ReduceByKey: V ⇒ V</p><ul id="15b2e7d3-01e5-80fe-808a-f41bfc3620ce" class="bulleted-list"><li style="list-style-type:disc">depends: <ul id="15b2e7d3-01e5-80c6-8072-e44da50215a0" class="bulleted-list"><li style="list-style-type:circle">reduces before shuffle ⇒ acts like combiner</li></ul><ul id="15b2e7d3-01e5-8069-8358-cc6f00fb69a3" class="bulleted-list"><li style="list-style-type:circle">Reduces after shuffle ⇒ acts like reducer</li></ul></li></ul><p id="15b2e7d3-01e5-80f7-bc9e-c1747f364d92" class="">Reduce: (K,V) ⇒ (K,V)[]</p><ul id="15b2e7d3-01e5-801c-bcbf-c172f1eacb2d" class="bulleted-list"><li style="list-style-type:disc">happens AFTER the shuffle</li></ul><p id="15b2e7d3-01e5-8012-a14c-e5cb029c807c" class="">
</p></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0">Overloading Driver</summary><div class="indented"><figure id="15b2e7d3-01e5-80df-8b9e-ca77c5e9ed16" class="image" style="text-align:center"><a href="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/f935a7b9-fc3c-4f4c-aadc-5604a2433c29.png"><img style="width:384px" src="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/f935a7b9-fc3c-4f4c-aadc-5604a2433c29.png"/></a></figure><figure id="15b2e7d3-01e5-80f7-8038-f771d4c19600" class="image" style="text-align:center"><a href="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-12-11_at_22.57.21.png"><img style="width:336px" src="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-12-11_at_22.57.21.png"/></a></figure><p id="15b2e7d3-01e5-80b9-90f3-de65529f74b3" class=""><strong>To Prevent overloading the driver memory: </strong></p><ol type="1" id="15b2e7d3-01e5-80e7-af5e-f97fc758c9a1" class="numbered-list" start="1"><li>Dont use <strong>actions </strong>w’ <strong>ounbounded output (e.g. </strong><code>countByKey</code>,but not <code>count</code>)</li></ol><ol type="1" id="15b2e7d3-01e5-8071-af38-fd7a7c7deaf2" class="numbered-list" start="2"><li>Use action like <code>saveAsTextFile</code> which goes to HDFS straight away, not the driver. </li></ol></div></details><hr id="11a2e7d3-01e5-80a0-acdc-e38f6165e48f"/><h1 id="15b2e7d3-01e5-80fa-aebd-db115eebab21" class="">Demos</h1><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0"><mark class="highlight-purple">Graphical Example</mark></summary><div class="indented"><figure id="15b2e7d3-01e5-808c-93bd-cf7ffb7a6f63" class="image"><a href="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-12-13_at_15.20.13.png"><img style="width:384px" src="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-12-13_at_15.20.13.png"/></a></figure><figure id="15b2e7d3-01e5-8080-9b61-f64e690868d9" class="image"><a href="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-12-13_at_15.20.29.png"><img style="width:384px" src="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-12-13_at_15.20.29.png"/></a></figure><figure id="15b2e7d3-01e5-807f-af83-f417a356c0cc" class="image"><a href="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-12-13_at_15.20.44.png"><img style="width:384px" src="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/Screenshot_2024-12-13_at_15.20.44.png"/></a></figure></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0"><mark class="highlight-purple"><strong>Example — Log Mining</strong></mark></summary><div class="indented"><p id="1142e7d3-01e5-8014-9dfa-fd711216dc48" class=""><strong>Goal:</strong></p><p id="1142e7d3-01e5-808d-9a02-ca399854a041" class="">Load error messages from a log file into memory for interactive pattern searching.</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1142e7d3-01e5-8082-88de-f257b6da09ad" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">lines = spark.textFile(“hdfs://...”)
errors = lines.filter(lambda s: s.startswith(“ERROR”)) # &lt;= transformed RDD
messages = errors.map(lambda s: s.split(“\t”)[2])      # transformed RDD

messages.cache() # hold messages RDD in MEM (of each worker)

# =&gt; Since the messages are cached, these are faster:
messages
.filter(lambda s: “mysql” in s)
.count() # action =&gt; triggers computation
messages
.filter(lambda s: “php” in s)
.count() # action!</code></pre><figure id="1142e7d3-01e5-80f1-80c3-ee0112ea5145" class="image" style="text-align:center"><a href="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/f3c99aff-efdb-436d-8fd0-9405330b53d6.png"><img style="width:192px" src="03%20%E2%80%94%20From%20MapReduce%20to%20Spark/f3c99aff-efdb-436d-8fd0-9405330b53d6.png"/></a><figcaption><strong>1 driver</strong> and <strong>3 worker nodes</strong>. </figcaption></figure><ul id="15b2e7d3-01e5-80cd-b1b6-ce56c3887000" class="bulleted-list"><li style="list-style-type:disc">The first execution of <code>count()</code> are slower because it must compute everything from scratch. Subsequent counts are faster due to cached data being readily available in memory.</li></ul><ul id="15b2e7d3-01e5-80bf-a1f7-d7976830b3d9" class="bulleted-list"><li style="list-style-type:disc">Each worker caches amd transforms their respective partitions of the <code>messages</code> RDD, then sends those counts back to the driver</li></ul><ul id="1142e7d3-01e5-80ac-b297-ea3a4331f46b" class="bulleted-list"><li style="list-style-type:disc">The driver sums up counts from worker</li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0"><mark class="highlight-purple">Basic Transformations</mark></summary><div class="indented"><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1142e7d3-01e5-80be-b80b-eba21a98b4eb" class="code"><code class="language-Python">nums = sc.parallelize([1, 2, 3])
# Pass each element through a functio
squares = nums.map(lambda x: x*x)   // {1, 4, 9}
# Keep elements passing a predicate
even = squares.filter(lambda x: x % 2 == 0) // {4}
# Map each element to zero or more others
nums.flatMap(lambda x: =&gt; range(x))
	# =&gt; {0, 0, 1, 0, 1, 2}</code></pre><ul id="1142e7d3-01e5-8096-9946-ed2e7ac2769c" class="bulleted-list"><li style="list-style-type:disc">flatmap doesnt need to be list or vector or etc... just needs to be traversable with a for loop (ie: range, string) </li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0"><mark class="highlight-purple">Basic Actions</mark></summary><div class="indented"><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1142e7d3-01e5-80dc-bf7b-dc1824ba14bc" class="code"><code class="language-Python">nums = sc.parallelize([1, 2, 3])
# Retrieve RDD contents as a local collection
nums.collect() # =&gt; [1, 2, 3]
# Return first K elements
nums.take(2)   # =&gt; [1, 2]
# Count number of elements
nums.count()   # =&gt; 3
# Merge elements with an associative function
nums.reduce(lambda x, y: x + y)  # =&gt; 6
# Write elements to a text file
nums.saveAsTextFile(“hdfs://file.txt”)</code></pre></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0"><mark class="highlight-purple">KVP Operations</mark></summary><div class="indented"><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1142e7d3-01e5-80f6-9ce6-cd0a373d7bb3" class="code"><code class="language-Python">pets = sc.parallelize(
	[(“cat”, 1), (“dog”, 1), (“cat”, 2)]
pets.reduceByKey(lambda x, y: x + y)
# =&gt; {(cat, 3), (dog, 1)}
pets.groupByKey()
# =&gt; {(cat, [1, 2]), (dog, [1])}
pets.sortByKey()
 # =&gt; {(cat, 1), (cat, 2), (dog, 1)}</code></pre></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0"><mark class="highlight-purple">KVP Specific Operations</mark></summary><div class="indented"><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1142e7d3-01e5-8091-9502-da30062ad91c" class="code"><code class="language-Python">visits = sc.parallelize([ (“index.html”, “1.2.3.4”),
(“about.html”, “3.4.5.6”), (“index.html”, “1.3.3.1”) ])

pageNames = sc.parallelize([ (“index.html”, “Home”),
(“about.html”, “About”) ])

visits.join(pageNames)
# (“index.html”, (“1.2.3.4”, “Home”))
# (“index.html”, (“1.3.3.1”, “Home”))
# (“about.html”, (“3.4.5.6”, “About”))

 visits.cogroup(pageNames)
# (“index.html”, ([“1.2.3.4”, “1.3.3.1”], [“Home”]))
# (“about.html”, ([“3.4.5.6”], [“About”]))</code></pre></div></details><hr id="15b2e7d3-01e5-8025-b44d-d77119498b6e"/><h3 id="1142e7d3-01e5-8051-9f66-d268f88503dc" class="">Problem — Running Forever</h3><ul id="1142e7d3-01e5-807d-bfa6-fb01ed2d1041" class="bulleted-list"><li style="list-style-type:disc">A big reason for “runs forever” on datasci is a reducer that’s O(n) – usually caused by stripes being merged inefficiently. <ul id="1142e7d3-01e5-808a-bbca-d8d226d1835f" class="bulleted-list"><li style="list-style-type:circle">map runs, then reduce runs (pretty linear), although combiner runs on both machines.</li></ul><ul id="1142e7d3-01e5-801e-846c-fe1d57a92756" class="bulleted-list"><li style="list-style-type:circle">infinite loop in mapper:setup(), reducer is frozen at 67% (acc at 0% of reducing progress)</li></ul></li></ul><h3 id="1142e7d3-01e5-80b2-8ac6-fa4297744890" class="">MapReduce Execution and Monitoring</h3><ul id="1142e7d3-01e5-80c0-8d00-fef70025981f" class="bulleted-list"><li style="list-style-type:disc"><strong>Mapper and Reducer Progress</strong>:<ul id="1142e7d3-01e5-8009-9ff3-e83feb40f3f0" class="bulleted-list"><li style="list-style-type:circle">When running a MapReduce job, you can monitor which part (Mapper or Reducer) is currently executing through a progress bar.</li></ul><ul id="1142e7d3-01e5-8059-bf52-d744a7c3fd2c" class="bulleted-list"><li style="list-style-type:circle">The Map phase runs first, followed by the Reducer.</li></ul></li></ul><ul id="1142e7d3-01e5-8008-8565-ff171c6d2bc3" class="bulleted-list"><li style="list-style-type:disc"><strong>Progress Indicators</strong>:<ul id="1142e7d3-01e5-8053-a91f-d78456e5d04a" class="bulleted-list"><li style="list-style-type:circle">If the Reducer reaches <strong>67%</strong>, it indicates that the Mappers have completed their task and the shuffle phase (which transmits intermediate files) is also finished.</li></ul><ul id="1142e7d3-01e5-8084-b62b-f71fa10ff935" class="bulleted-list"><li style="list-style-type:circle">The shuffle stage is crucial, as it prepares and merges intermediate data for the Reducer.</li></ul></li></ul><ul id="1142e7d3-01e5-8030-bff0-ce218680bd28" class="bulleted-list"><li style="list-style-type:disc"><strong>Job Stages</strong>:<ul id="1142e7d3-01e5-8042-93ab-f972d7aaa20e" class="bulleted-list"><li style="list-style-type:circle">There are multiple stages beyond just Map and Reduce. A complex job may require analyzing where the bottleneck is occurring (e.g., during the shuffle or reduction).</li></ul></li></ul><ul id="1142e7d3-01e5-8099-80e8-d0659c532c0f" class="bulleted-list"><li style="list-style-type:disc"><strong>Directed Acyclic Graph (DAG)</strong>:<ul id="1142e7d3-01e5-807a-abf9-ee32e38c2f7b" class="bulleted-list"><li style="list-style-type:circle">The job&#x27;s DAG representation shows how stages and transformations (like tokenization and counting) are connected. More complex jobs will have a more intricate DAG.</li></ul></li></ul></div></article><span class="sans" style="font-size:14px;padding-top:2em"><hr/><details open="" style="padding-top:1em"><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0">Inline comments</summary><div class="indented"><div><p style="padding:0.2em"><b>Block text</b>: <mark class="highlight-yellow_background">Similar to <mark class="highlight-gray"><code>map</code></mark>, but runs separately for each partition (block) of the RDD</mark></p><ul class="toggle"><li><div><span class="user" style="padding:0.2em"><img src="https://lh3.googleusercontent.com/a/ALm5wu2D7opyGPhg1IC5ZBqM5ZYIOR8MwAIakj3yS20lKw=s100" class="icon user-icon"/><span><b>roman hudaj</b> <time style="font-size:0.8em">Oct 9, 2024, 2:00 PM</time></span></span></div><div style="padding:0.2em"><mark class="highlight-default">similar to </mark><mark class="highlight-default"><strong>MapReduce</strong></mark><mark class="highlight-default">&#x27;s idea of working on data partitions with setup and cleanup phases but allows a broader range of operations to be performed on the data.</mark></div><div style="padding:0.3em"></div></li></ul></div><hr/></div><div class="indented"><div><p style="padding:0.2em"><b>Block text</b>: <mark class="highlight-yellow_background">Repartition the RDD according to the partitioner, sort by keys within each partition. </mark></p><ul class="toggle"><li><div><span class="user" style="padding:0.2em"><img src="https://lh3.googleusercontent.com/a/ALm5wu2D7opyGPhg1IC5ZBqM5ZYIOR8MwAIakj3yS20lKw=s100" class="icon user-icon"/><span><b>roman hudaj</b> <time style="font-size:0.8em">Oct 9, 2024, 2:02 PM</time></span></span></div><div style="padding:0.2em">faster than <code>.repartition()</code> + sort within each partition</div><div style="padding:0.3em"></div></li></ul></div><hr/></div><div class="indented"><div><p style="padding:0.2em"><b>Block text</b>: <mark class="highlight-yellow_background">Similar to <mark class="highlight-gray"><code>map</code></mark>, but runs separately for each partition (block) of the RDD</mark></p><ul class="toggle"><li><div><span class="user" style="padding:0.2em"><img src="https://lh3.googleusercontent.com/a/ALm5wu2D7opyGPhg1IC5ZBqM5ZYIOR8MwAIakj3yS20lKw=s100" class="icon user-icon"/><span><b>roman hudaj</b> <time style="font-size:0.8em">Oct 9, 2024, 2:00 PM</time></span></span></div><div style="padding:0.2em"><mark class="highlight-default">similar to </mark><mark class="highlight-default"><strong>MapReduce</strong></mark><mark class="highlight-default">&#x27;s idea of working on data partitions with setup and cleanup phases but allows a broader range of operations to be performed on the data.</mark></div><div style="padding:0.3em"></div></li></ul></div><hr/></div><div class="indented"><div><p style="padding:0.2em"><b>Block text</b>: <mark class="highlight-yellow_background">Repartition the RDD according to the partitioner, sort by keys within each partition. </mark></p><ul class="toggle"><li><div><span class="user" style="padding:0.2em"><img src="https://lh3.googleusercontent.com/a/ALm5wu2D7opyGPhg1IC5ZBqM5ZYIOR8MwAIakj3yS20lKw=s100" class="icon user-icon"/><span><b>roman hudaj</b> <time style="font-size:0.8em">Oct 9, 2024, 2:02 PM</time></span></span></div><div style="padding:0.2em">faster than <code>.repartition()</code> + sort within each partition</div><div style="padding:0.3em"></div></li></ul></div><hr/></div></details><details open="" style="padding-top:1em"><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0">Resolved inline comments</summary><div class="indented"><div><p style="padding:0.2em"><b>Block text</b>: <mark class="highlight-yellow_background">visits = <mark class="highlight-orange">load</mark> ‘/data/visits’ as (user, url, time); </mark></p><ul class="toggle"><li><div><span class="user" style="padding:0.2em"><img src="https://lh3.googleusercontent.com/a/ALm5wu2D7opyGPhg1IC5ZBqM5ZYIOR8MwAIakj3yS20lKw=s100" class="icon user-icon"/><span><b>roman hudaj</b> <time style="font-size:0.8em">Oct 2, 2024, 8:54 PM</time></span></span></div><div style="padding:0.2em">done by map</div><div style="padding:0.3em"></div></li></ul></div><hr/></div><div class="indented"><div><p style="padding:0.2em"><b>Block text</b>: <mark class="highlight-yellow_background">gVisits = <mark class="highlight-orange">group</mark> visits <mark class="highlight-orange">by</mark> url; </mark></p><ul class="toggle"><li><div><span class="user" style="padding:0.2em"><img src="https://lh3.googleusercontent.com/a/ALm5wu2D7opyGPhg1IC5ZBqM5ZYIOR8MwAIakj3yS20lKw=s100" class="icon user-icon"/><span><b>roman hudaj</b> <time style="font-size:0.8em">Oct 2, 2024, 8:55 PM</time></span></span></div><div style="padding:0.2em">group by requires shuffle (url is key)</div><div style="padding:0.3em"></div></li></ul></div><hr/></div></details></span></body></html>