<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>L08c</title><link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/></head><body><article id="1442e7d3-01e5-8013-b457-e43dc1b2256e" class="page sans"><header><div class="page-header-icon undefined"><img class="icon" src="https://www.notion.so/icons/document_pink.svg"/></div><h1 class="page-title">L08c</h1><p class="page-description"></p></header><div class="page-body"><p id="1442e7d3-01e5-801b-a3c3-d0ace7b05f01" class=""><strong>Topic: </strong>Unsupervised ML</p><hr id="14f2e7d3-01e5-8033-8d75-c8c3bd7bc11d"/><p id="14f2e7d3-01e5-801f-a7a4-fc6191a10bc6" class=""><strong>Questions</strong></p><ul id="14f2e7d3-01e5-80ba-b15b-e89c492f67fb" class="bulleted-list"><li style="list-style-type:disc">Does this only apply to cases where <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Z</mi></mrow><annotation encoding="application/x-tex">Z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span></span></span></span></span><span>﻿</span></span> is only partially missing? </li></ul><hr id="1442e7d3-01e5-800f-90a6-fc25f5bb47ea"/><p id="1442e7d3-01e5-80ef-9aaf-f4810d7a96af" class="">Many real-world problems have <strong>hidden (aka latent) variables </strong>meaning <strong>incomplete data</strong> —Values of some attributes missing. </p><ul id="1442e7d3-01e5-8051-b6f8-f156f50d2223" class="bulleted-list"><li style="list-style-type:disc">e.g. some (or all) data is unlabeled </li></ul><ul id="1442e7d3-01e5-8045-b3af-d3b25f883a31" class="bulleted-list"><li style="list-style-type:disc">e.g. some variable values are missing </li></ul><p id="1442e7d3-01e5-8001-8270-f8db1d21e37f" class=""><strong>Unsupervised Learning</strong> solves these types of problems. </p><hr id="1442e7d3-01e5-8093-850d-f4a13f8324ac"/><h3 id="14f2e7d3-01e5-8046-b10d-ddf81a6b4728" class="">MLE Does not Work with Missing Data </h3><p id="1442e7d3-01e5-800d-bda4-e7f46f42cd94" class="">Recall, MLE: </p><figure id="1442e7d3-01e5-80dd-922a-dd3998bac5d2" class="equation"><link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>θ</mi><mrow><mi>V</mi><mo separator="true">,</mo><mi>p</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>t</mi><mo stretchy="false">(</mo><mi>V</mi><mo stretchy="false">)</mo></mrow></msub><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mi>V</mi><mi mathvariant="normal">∣</mi><mi>p</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>t</mi><mo stretchy="false">(</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mi>v</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\theta_{V,parent(V)}=P(V|parent(V)=v)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0496em;vertical-align:-0.3552em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">re</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">t</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord">∣</span><span class="mord mathnormal">p</span><span class="mord mathnormal">a</span><span class="mord mathnormal">re</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mclose">)</span></span></span></span></span></div></figure><figure id="1442e7d3-01e5-8038-b19f-f16a22554650" class="equation"><link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>θ</mi><mrow><mi>M</mi><mi>L</mi><mi>E</mi></mrow></msub><mo>=</mo><mfrac><mrow><mrow><mstyle mathcolor="#df0030"><mtext>#</mtext></mstyle><mtext> with (</mtext><mstyle scriptlevel="0" displaystyle="false"><mi>V</mi><mo>=</mo><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mo>∧</mo><mi>p</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>s</mi><mo stretchy="false">(</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mi>v</mi></mstyle></mrow><mo stretchy="false">)</mo></mrow><mrow><mrow><mstyle mathcolor="#df0030"><mtext>#</mtext></mstyle><mtext> with (parents(V)=v</mtext></mrow><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\theta_{MLE}=
\frac
{\text{\red{\#} with ($V=true \land parents(V)=v$})}
{\text{\red{\#} with (parents(V)=v})}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span><span class="mord mathnormal mtight">L</span><span class="mord mathnormal mtight" style="margin-right:0.05764em;">E</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord" style="color:#df0030;">#</span><span class="mord"> with (parents(V)=v</span></span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord" style="color:#df0030;">#</span><span class="mord"> with (</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">u</span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∧</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">a</span><span class="mord mathnormal">re</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">s</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div></figure><p id="1442e7d3-01e5-808a-93f1-c738e4c14e74" class="">If we’re <strong>missing some data</strong>, we CAN’T <mark class="highlight-red">count these numbers</mark>. </p><ul id="1442e7d3-01e5-8073-a2e4-e1a887d515f6" class="toggle"><li><details open=""><summary><mark class="highlight-purple"><strong>Example</strong></mark></summary><figure id="1442e7d3-01e5-805c-a5bf-c210d9ba68cc" class="image" style="text-align:center"><a href="L08c/Screenshot_2024-11-20_at_18.48.13.png"><img style="width:432px" src="L08c/Screenshot_2024-11-20_at_18.48.13.png"/></a><figcaption>Incomplete data. </figcaption></figure></details></li></ul><h2 id="1442e7d3-01e5-800a-85e8-ecf7e4e33ec9" class="">Exploring Options</h2><ul id="1442e7d3-01e5-80c7-8de5-c61a99fcdab6" class="toggle"><li><details open=""><summary>Ignore the hidden variable ❌</summary><figure id="1442e7d3-01e5-80af-937f-d68c505817bd" class="image" style="text-align:center"><a href="L08c/Screenshot_2024-11-20_at_18.50.17.png"><img style="width:384px" src="L08c/Screenshot_2024-11-20_at_18.50.17.png"/></a><figcaption>b) Is a model that connect factors to symptoms without considering the heart-disease variable ⇒ latent variable. </figcaption></figure><p id="1452e7d3-01e5-806e-88d1-e91348548bb1" class="">Problems: </p><ul id="1442e7d3-01e5-8037-8112-e0cc27f60509" class="bulleted-list"><li style="list-style-type:disc">Result has much more parameters. </li></ul><ul id="1442e7d3-01e5-80c4-b97b-c7bea4327a0c" class="bulleted-list"><li style="list-style-type:disc">Also ignores what we want to know: do they have the disease </li></ul></details></li></ul><ul id="1442e7d3-01e5-8005-af2c-ea81a48683bb" class="toggle"><li><details open=""><summary>Ignore Data that’s Missing Values ❌</summary><ul id="1442e7d3-01e5-8023-bb53-e57c1ee3c8dc" class="bulleted-list"><li style="list-style-type:disc">does not work with <strong>true</strong> <strong>latent</strong> <strong>variables</strong> (e.g. the data is always missing)</li></ul><ul id="1442e7d3-01e5-804d-b0af-e195e5889377" class="bulleted-list"><li style="list-style-type:disc"><strong>Can’t ignore missing data </strong>unless<strong> YOU KNOW </strong>it’s <strong>missing at RANDOM</strong>. Otherwise the reason data is missing is correlated with a variable of interest (i.e. what you want to predict). </li></ul><ul id="1442e7d3-01e5-80ad-8691-cd6aed091221" class="toggle"><li><details open=""><summary><mark class="highlight-purple"><strong>Example</strong></mark></summary><p id="1442e7d3-01e5-80d4-9dfd-c9fd670ac3bf" class="">data in a clinical trial to test a drug may be missing because:</p><ul id="1442e7d3-01e5-808f-b5fd-f37fa4ed9513" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-red">the patient dies</mark></li></ul><ul id="1442e7d3-01e5-8018-97e3-f10e4f4e5750" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-red">the patient dropped out because of severe side effects</mark></li></ul><ul id="1442e7d3-01e5-8071-a542-d1b325748592" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-teal">they dropped out because they were better</mark></li></ul><ul id="1442e7d3-01e5-806c-a0d3-dc4651a85d35" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-teal">the patient had to visit a sick relative.</mark></li></ul><p id="1442e7d3-01e5-80ba-8ce0-d32ae0fc73b9" class="">Ignoring some of these may make the drug look <mark class="highlight-red">better</mark> or <mark class="highlight-teal">worse</mark> than it is.</p></details></li></ul></details></li></ul><ul id="1442e7d3-01e5-800a-9618-f05ea70c67c7" class="toggle"><li><details open=""><summary>Direct MLE ❌</summary><p id="1442e7d3-01e5-807c-a9be-c539502e6a39" class="">If <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span></span></span></span></span><span>﻿</span></span> is the observable data, and there is a hidden variable <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Z</mi></mrow><annotation encoding="application/x-tex">Z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span></span></span></span></span><span>﻿</span></span>, then using MLE, we would end up with an expression that is tough to simplify/optimize because we can’t linearize it.</p><figure id="1452e7d3-01e5-80e3-ad16-ffb08b687bcf" class="image" style="text-align:center"><a href="L08c/IMG_E31FE0226525-1.jpeg"><img style="width:384px" src="L08c/IMG_E31FE0226525-1.jpeg"/></a><figcaption><link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">h</span></span></span></span></span><span>﻿</span></span> represents the parameters</figcaption></figure></details></li></ul><h3 id="14f2e7d3-01e5-8005-9f52-d6d94d982720" class="">Expectation Maximization <mark class="highlight-teal">✅</mark></h3><p id="14f2e7d3-01e5-802b-a0e6-fb64cfc9cda1" class=""><em><mark class="highlight-gray"><strong>How to find find maximum-likelihood parameters when missing data</strong></mark></em></p><p id="14f2e7d3-01e5-800a-a1f8-f68f23c879b8" class="">Algorithm to approximate the maximum likelihood</p><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="14f2e7d3-01e5-80d0-8999-e131803484a4"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="09422ee0-ab28-4066-a2f3-a9fde7978f49" class="">Recall: if we have the joint distribution, we can answer any question. However, we don’t have the full distribution due to some hidden variable(s). Instead of computing joint-pdf, we find the expectation of it w’ respect to the hidden variable. Then we maximize it over <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">h</span></span></span></span></span><span>﻿</span></span> (the parameters). </p><p id="14f2e7d3-01e5-8049-93fc-c01a45331ff7" class="">If we knew the missing values, computing MLE estimate for parameters would be easy again. Instead of counting data points, we sum their <strong>weights </strong>(posterior probability of the missing data / hidden variables). Then we compute the most likely values for the data, then do MLE.</p><p id="14f2e7d3-01e5-80b2-87ac-f98d2384d17d" class="">Can think of EM as “<strong>Augmented Data Method</strong>”; use the posterior as <strong>partial data </strong>in ML learning. </p></div></figure><figure class="block-color-default_background callout" style="white-space:pre-wrap;display:flex" id="14f2e7d3-01e5-809b-ba48-f2979695798c"><div style="font-size:1.5em"><img class="icon" src="https://www.notion.so/icons/exclamation-mark_gray.svg"/></div><div style="width:100%"><p id="76f35d3d-bf8c-4cd3-888f-9d71617f646d" class=""><mark class="highlight-red"><strong>EM Algorithm </strong></mark></p><hr id="14f2e7d3-01e5-8064-9189-c840d09889ca"/><figure id="1562e7d3-01e5-8083-a842-ff61d86f25af" class="image"><a href="L08c/IMG_9E1F199F4296-1.jpeg"><img style="width:646.748046875px" src="L08c/IMG_9E1F199F4296-1.jpeg"/></a></figure></div></figure><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="14f2e7d3-01e5-8012-aac5-e478d6246547"><div style="font-size:1.5em"><img class="icon" src="https://www.notion.so/icons/exclamation-mark_gray.svg"/></div><div style="width:100%"><p id="31a18eb9-741c-4515-bd05-58ee20c0f91f" class="">EM <strong>always increases the likelihood</strong></p><figure id="14f2e7d3-01e5-80d3-9427-ff003c0f13ea" class="equation"><link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>e</mi><mi mathvariant="normal">∣</mi><msub><mi>h</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo>≥</mo><mi>P</mi><mo stretchy="false">(</mo><mi>e</mi><mi mathvariant="normal">∣</mi><msub><mi>h</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(e|h_{i+1}) \ge P(e|h_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">e</span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">e</span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></div></figure><p id="14f2e7d3-01e5-80bc-8b97-ebbaf0fe74bd" class="">It always find a local optimum, but not necessarily a global optimum. It depends on what our initial guess is. So the initial guess is very important (won’t cover). </p></div></figure><hr id="14f2e7d3-01e5-800a-a329-c4fc33528453"/><ul id="1452e7d3-01e5-80df-b2d5-d749ef4faadd" class="toggle"><li><details open=""><summary><strong>(aside) </strong><link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span></span><span>﻿</span></span><strong>-means</strong></summary><figure class="block-color-default_background callout" style="white-space:pre-wrap;display:flex" id="1452e7d3-01e5-8040-a40e-d127af4032eb"><div style="font-size:1.5em"><img class="icon" src="https://www.notion.so/icons/code_gray.svg"/></div><div style="width:100%"><p id="91c335f0-e874-40e3-9240-ce6aff06b6ee" class="">K-Means</p><p id="1452e7d3-01e5-806a-952d-d9667491f390" class="">Input: <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span></span></span></span></span><span>﻿</span></span>, # classes <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span></span><span>﻿</span></span></p><p id="1452e7d3-01e5-8040-b8bf-edbd84964a03" class="">Output: A value for each input feature for each class, and an assignment of examples to classes. </p><hr id="1452e7d3-01e5-8018-a811-ef1a8a37d138"/><ol type="1" id="1452e7d3-01e5-80ac-8747-f0217f4d8343" class="numbered-list" start="1"><li>Pick <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span></span><span>﻿</span></span> means in <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span></span><span>﻿</span></span></li></ol><ol type="1" id="1452e7d3-01e5-8090-9927-de57cfa1fc68" class="numbered-list" start="2"><li>Iterate (until means stop changing)<ol type="a" id="1452e7d3-01e5-8025-9364-c90e656471b0" class="numbered-list" start="1"><li>assign examples to <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span></span><span>﻿</span></span>-classes (closest to current means)</li></ol><ol type="a" id="1452e7d3-01e5-8079-b28f-fb459547be77" class="numbered-list" start="2"><li>re-estimate <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span></span><span>﻿</span></span>-means based on assigment</li></ol></li></ol></div></figure><ul id="1452e7d3-01e5-80ee-948d-fae84b3eefe6" class="bulleted-list"><li style="list-style-type:disc">Specific instance of the EM-Algorithm</li></ul><ul id="14f2e7d3-01e5-8083-8fb3-ee784267384e" class="bulleted-list"><li style="list-style-type:disc">The only difference is in the <strong>E-step</strong>: Compute <strong>most likely </strong>missing values for the hidden variables (i.e. instead of a distribution for missing values, you take only the most likely). <ul id="14f2e7d3-01e5-809a-bdce-ea323f1cc189" class="bulleted-list"><li style="list-style-type:circle">⇒ Simpler because we essentially have <em>complete data (chooses most likely)</em>. In EM, it’s <em>partial </em>since it’s a distribution of most likely. </li></ul></li></ul><ul id="1452e7d3-01e5-80ed-9644-da7a6216e824" class="bulleted-list"><li style="list-style-type:disc">This is commonly used as a <em>clustering algorithm </em>(not shown here). </li></ul></details></li></ul><hr id="14f2e7d3-01e5-808a-9f70-d7097d7fd873"/><h2 id="1452e7d3-01e5-80f4-8f16-ca0ac7704047" class="">EM for Naive Bayes</h2><figure id="14f2e7d3-01e5-80a0-acdf-dbae661fb1bd" class="image"><a href="L08c/IMG_76D74814BAFF-1.jpeg"><img style="width:576px" src="L08c/IMG_76D74814BAFF-1.jpeg"/></a></figure><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0"><mark class="highlight-purple"><strong>Example  1 </strong></mark></summary><div class="indented"><figure id="1452e7d3-01e5-8055-a611-ce5ea4d90f88" class="image" style="text-align:center"><a href="L08c/IMG_997C96D4B2B1-1.jpeg"><img style="width:432px" src="L08c/IMG_997C96D4B2B1-1.jpeg"/></a></figure></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0"><mark class="highlight-purple"><strong>Example 2</strong></mark></summary><div class="indented"><figure id="1562e7d3-01e5-801e-ac3f-cb67d95ccef3" class="image"><a href="L08c/IMG_78D2EFFDDC20-1.jpeg"><img style="width:1410px" src="L08c/IMG_78D2EFFDDC20-1.jpeg"/></a></figure><p id="1452e7d3-01e5-80c4-bbd1-ddfbbc9214b5" class="">
</p></div></details><h3 id="14f2e7d3-01e5-808a-80aa-eafebca8e221" class="">Summary</h3><figure id="1452e7d3-01e5-8071-98c8-c41465b2ba4e" class="image"><a href="L08c/IMG_0FEF17879663-1.jpeg"><img style="width:680.0000610351562px" src="L08c/IMG_0FEF17879663-1.jpeg"/></a></figure><p id="1452e7d3-01e5-80a5-8ab0-f71e0df24c06" class="">
</p><h2 id="1452e7d3-01e5-8030-b20f-eae046685410" class="">Learning the Structure of a B.N</h2><p id="1452e7d3-01e5-8017-893a-fb346e466876" class="">In a B.N, the number of classes, <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span></span><span>﻿</span></span>, is part of the structure of the network. </p><p id="1452e7d3-01e5-8069-91a1-ec0ed8513929" class="">We can use the same ML methods to get these. </p><figure id="1452e7d3-01e5-804e-a7a1-c0f630edff76" class="equation"><link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi><mi mathvariant="normal">∣</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mi mathvariant="normal">∣</mi><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi><mo stretchy="false">)</mo><mi>P</mi><mo stretchy="false">(</mo><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(model|data)=\frac{P(data|model)P(model)}{P(data)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">m</span><span class="mord mathnormal">o</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord">∣</span><span class="mord mathnormal">d</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">d</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">d</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord">∣</span><span class="mord mathnormal">m</span><span class="mord mathnormal">o</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">m</span><span class="mord mathnormal">o</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div></figure><ul id="1452e7d3-01e5-8019-a537-cac0bc336f4d" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-purple">e.g. How do we learn the value of </mark><mark class="highlight-purple"><link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span></span><span>﻿</span></span></mark><mark class="highlight-purple"> in k-means? </mark></li></ul><ul id="14f2e7d3-01e5-80a4-8145-ee5e30445cfe" class="bulleted-list"><li style="list-style-type:disc">For example, you have a prior over <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span></span><span>﻿</span></span>, you can estimate the likelihood of dataset given <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span></span><span>﻿</span></span>, giving you probability for that partitcular model. Do that for all values of <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span></span><span>﻿</span></span> for a distribution. </li></ul><h2 id="1452e7d3-01e5-809b-8ef1-e87bc04934aa" class="">Alternatives</h2><p id="1452e7d3-01e5-8089-826d-d5fb99c83c03" class="">skipped (slides 19-26)</p><ul id="1452e7d3-01e5-80bf-b235-e8d1f92479fb" class="bulleted-list"><li style="list-style-type:disc">autoencoders</li></ul><ul id="1452e7d3-01e5-80b7-bff1-f62e7bf22a7e" class="bulleted-list"><li style="list-style-type:disc">GANs </li></ul></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>