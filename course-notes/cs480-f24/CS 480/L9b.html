<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>L9b</title><link rel="stylesheet" type="text/css" href="notion_styles.css"/></head><body><article id="12a2e7d3-01e5-8011-8252-d2bd5117fbe4" class="page sans"><header><div class="page-header-icon undefined"><img class="icon" src="https://www.notion.so/icons/document_blue.svg"/></div><h1 class="page-title">L9b</h1><p class="page-description"></p></header><div class="page-body"><p id="1382e7d3-01e5-8079-abf6-dfaeb794fa1c" class=""><strong>Topic: </strong>Gaussian Process Model</p><hr id="1382e7d3-01e5-80b0-b8d4-fe6ce35d41e2"/><p id="12d2e7d3-01e5-8067-8a54-f5d230cb1407" class="">In L09a, we knew what the <mark class="highlight-pink">true underlying function </mark>looked like, since we knew the set of non-linear basis functions. We looked at how we could incorporate prior knowledge. </p><figure id="12d2e7d3-01e5-802b-9c48-cf75e73ac356" class="image"><a href="L9b/Screenshot_2024-10-28_at_17.57.53.png"><img style="width:288px" src="L9b/Screenshot_2024-10-28_at_17.57.53.png"/></a><figcaption>The <mark class="highlight-pink">true underlying function </mark>that produces the observed data points (+). </figcaption></figure><p id="12d2e7d3-01e5-80fa-9bed-d1e47154e1e5" class="">This lecture: </p><ul id="12d2e7d3-01e5-8037-a6a1-f693f786aef7" class="bulleted-list"><li style="list-style-type:disc">We dont know the model (set of basis functions)</li></ul><ul id="12d2e7d3-01e5-8063-959e-d6ec84418133" class="bulleted-list"><li style="list-style-type:disc">We will find a distribution of possible functions. (ie: a more flexible model, that can capture any underlying function that passes through these points). </li></ul><h3 id="12d2e7d3-01e5-8037-8e06-d4e1b403ff71" class="">Gaussian Process Model</h3><p id="12d2e7d3-01e5-805f-b94d-c724dae4b766" class="">Non-Parametric Model: Learns <span style="border-bottom:0.05em solid">distributions over </span><em><span style="border-bottom:0.05em solid">functions</span></em>. Allows us to do regression without assuming a specific set of basis functions. </p><figure id="12d2e7d3-01e5-809a-b1c3-f5d998c867a7" class="image"><a href="L9b/Screenshot_2024-10-28_at_18.04.15.png"><img style="width:384px" src="L9b/Screenshot_2024-10-28_at_18.04.15.png"/></a><figcaption>This shows the gaussian-process model <strong>after we’ve trained it. </strong>We’re trying to do non-linear regression, but the true underlying model is unknown. </figcaption></figure><ul id="12d2e7d3-01e5-800e-860d-ed0d6df9032e" class="bulleted-list"><li style="list-style-type:disc">The process learns a <strong>mean function </strong></li></ul><ul id="12d2e7d3-01e5-80b9-9329-e2404125f099" class="bulleted-list"><li style="list-style-type:disc">It has <strong>uncertainty</strong> along the mean, <link rel="stylesheet" type="text/css" href="notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\sigma^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span><ul id="12d2e7d3-01e5-8065-b047-d3729c4bb2b4" class="bulleted-list"><li style="list-style-type:circle">low near data points </li></ul><ul id="12d2e7d3-01e5-80dd-b4cf-fef106081aac" class="bulleted-list"><li style="list-style-type:circle">high when no points to go off of. </li></ul></li></ul><hr id="12d2e7d3-01e5-8002-8a37-c6767cad3b67"/><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="1382e7d3-01e5-80a5-b140-f2a23396b959"><div style="font-size:1.5em"><img class="icon" src="https://www.notion.so/icons/exclamation-mark-double_gray.svg"/></div><div style="width:100%"><p id="9104969f-2065-4dc2-9533-558d5f2edf49" class=""><strong>“Distributions over Functions” — what that means? </strong></p><p id="12d2e7d3-01e5-8046-82ec-feffe3087015" class=""><strong>Property</strong></p><blockquote id="1382e7d3-01e5-8064-8ce5-e3359cfa9fbd" class="">A linear combination of gaussian-distributed-variables is itself, gaussian distributed. </blockquote><p id="1382e7d3-01e5-807c-95a7-d3107c3f90f8" class=""><em>⇒ the distribution of the function values for a single point are gaussian </em> </p><ul id="12d2e7d3-01e5-8095-829d-db439bd9c67b" class="toggle"><li><details open=""><summary>Details</summary><p id="12d2e7d3-01e5-804a-839b-fabc1686bebe" class="">In L09a, instead of finding a single point-estimate of <link rel="stylesheet" type="text/css" href="notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span></span><span>﻿</span></span> to define a function, we found a distribution over all possible <link rel="stylesheet" type="text/css" href="notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span></span><span>﻿</span></span>. </p><figure id="12d2e7d3-01e5-8073-9e88-db045e848e5d" class="image" style="text-align:center"><a href="L9b/Screenshot_2024-10-28_at_18.09.32.png"><img style="width:480px" src="L9b/Screenshot_2024-10-28_at_18.09.32.png"/></a><figcaption>A) posterior distribution of weights, B) model from a specific weight sample, C) shows the model’s prediction for a specific value of <link rel="stylesheet" type="text/css" href="notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mn>3.7</mn></mrow><annotation encoding="application/x-tex">x=3.7</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">3.7</span></span></span></span></span><span>﻿</span></span></figcaption></figure><figure id="12d2e7d3-01e5-8009-9359-c6ef4ae886db" class="image" style="text-align:center"><a href="L9b/Screenshot_2024-10-28_at_18.16.26.png"><img style="width:480px" src="L9b/Screenshot_2024-10-28_at_18.16.26.png"/></a><figcaption>After doing this many times.</figcaption></figure><p id="12d2e7d3-01e5-80d0-89e5-f20102b89e57" class="">Each set of weights came from a gaussian distribution. The resulting function values also follow a gaussian distribution. </p><p id="1382e7d3-01e5-80f9-8cac-f7252cb98a47" class="">By repeating this for every <link rel="stylesheet" type="text/css" href="notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span></span><span>﻿</span></span> value, we get this plot: </p><figure id="12d2e7d3-01e5-80f6-8c7c-ddb46cc790ff" class="image" style="text-align:center"><a href="L9b/Screenshot_2024-10-28_at_18.04.15.png"><img style="width:288px" src="L9b/Screenshot_2024-10-28_at_18.04.15.png"/></a></figure></details></li></ul><p id="12d2e7d3-01e5-80f4-8fe6-c9fc7bbbaf3c" class="">⇒ The joint-distribution between any <strong>2 input points </strong>is also Gaussian. </p><ul id="12d2e7d3-01e5-8022-bd0d-fb8634460366" class="toggle"><li><details open=""><summary>Details</summary><figure id="12d2e7d3-01e5-80bd-aa65-f35f966031b4" class="image" style="text-align:center"><a href="L9b/Screenshot_2024-10-28_at_18.22.08.png"><img style="width:336px" src="L9b/Screenshot_2024-10-28_at_18.22.08.png"/></a><figcaption>Each of the function values for a specific <link rel="stylesheet" type="text/css" href="notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span></span><span>﻿</span></span> value, follow a G.D. The middle plot shows the joint-prob-density, which comes from multiplying both of them. which is max at the mean value for both inputs. </figcaption></figure></details></li></ul><p id="12d2e7d3-01e5-8079-910a-d2a719804ded" class="">⇒ The join-distribution between any<strong> # input points</strong> is also Gaussian (ie: the function values for any set of points follows a <em>multivariate gaussian distribution</em>). </p><hr id="1382e7d3-01e5-8081-b528-e44e6df2320c"/><p id="1382e7d3-01e5-80b1-aa4e-ea07ca7593e0" class=""><strong>Property </strong></p><blockquote id="1382e7d3-01e5-8023-856c-cc9e0b688157" class="">A gaussian processes is completely specified by <link rel="stylesheet" type="text/css" href="notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo separator="true">,</mo><mi mathvariant="bold">Σ</mi></mrow><annotation encoding="application/x-tex">\mu, \bold{\Sigma}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8805em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">μ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathbf">Σ</span></span></span></span></span><span>﻿</span></span></blockquote><p id="1382e7d3-01e5-80b6-b967-c6574887e58e" class=""><em>⇒ S</em>o is the<em> joint-distribution over function values </em></p></div></figure><p id="12d2e7d3-01e5-8031-b67f-e15d1c650ab0" class=""><strong>Deriving </strong><link rel="stylesheet" type="text/css" href="notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo separator="true">,</mo><mi mathvariant="bold">Σ</mi></mrow><annotation encoding="application/x-tex">\mu, \bold{\Sigma}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8805em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">μ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathbf">Σ</span></span></span></span></span><span>﻿</span></span><strong> — For the Linear Model of N.L.B.F’s </strong><em><strong> </strong></em></p><figure id="1382e7d3-01e5-80e2-8990-fd5d902f0ed1" class="image" style="text-align:center"><a href="L9b/IMG_4020A498022D-1.jpeg"><img style="width:432px" src="L9b/IMG_4020A498022D-1.jpeg"/></a><figcaption>Based on our assumptions, the covariance matrix completely specifies the joint distribution, and it can be computed through the kernel. </figcaption></figure><hr id="12d2e7d3-01e5-8044-bfdb-c988a581072e"/><h3 id="12d2e7d3-01e5-8082-b06b-c88141b043b6" class=""><strong>G.P.M applied to Linear Regression</strong></h3><figure id="1382e7d3-01e5-80ac-8738-f73488dd9824" class="image"><a href="L9b/IMG_36F233952076-1.jpeg"><img style="width:480px" src="L9b/IMG_36F233952076-1.jpeg"/></a></figure><p id="12d2e7d3-01e5-8060-939d-ea763f7d0b29" class=""><link rel="stylesheet" type="text/css" href="notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span></span><span>﻿</span></span> = The sum of covariances of the <strong>2 sources of randomness. </strong>Thus, our uncertainty in <link rel="stylesheet" type="text/css" href="notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span></span><span>﻿</span></span> is from: </p><ol type="1" id="12d2e7d3-01e5-80f1-a8d4-fcab14c82d0f" class="numbered-list" start="1"><li>The underlying function uncertainty, and</li></ol><ol type="1" id="12d2e7d3-01e5-80cd-87e7-c781733f0542" class="numbered-list" start="2"><li>The noise in our observations </li></ol><hr id="12d2e7d3-01e5-8034-8576-d240860bc46c"/><h3 id="1382e7d3-01e5-8034-8d2b-ded997b34581" class=""><strong>Predict the Function for a Test Point </strong></h3><p id="1382e7d3-01e5-803a-8bb0-daba5f2f72cc" class="">The joint-distribution of observed training points allows us to describe how known points are related to each-other. </p><p id="1382e7d3-01e5-80ca-ac93-e02d334574af" class="">Our goal is to predict the function at a new, unseen test point. </p><hr id="1382e7d3-01e5-80d0-a5ae-ff330524de6f"/><p id="12d2e7d3-01e5-804c-861a-d3f990e8437e" class=""><strong>Extending the gaussian process to make predictions: </strong></p><p id="12d2e7d3-01e5-80e4-97cf-f6be077aa668" class="">If the gaussian-distribution holds for <link rel="stylesheet" type="text/css" href="notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span></span><span>﻿</span></span> training points, it  should also hold for <link rel="stylesheet" type="text/css" href="notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">N+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></span><span>﻿</span></span>.</p><p id="12d2e7d3-01e5-808e-b636-d80e55cddf7f" class="">Augment <link rel="stylesheet" type="text/css" href="notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span></span><span>﻿</span></span>  (“absorb” the test point into the model). </p><figure id="1382e7d3-01e5-8017-b185-efb4b2ed5742" class="image"><a href="L9b/IMG_A485B68C55F0-1.jpeg"><img style="width:480px" src="L9b/IMG_A485B68C55F0-1.jpeg"/></a></figure><hr id="1382e7d3-01e5-80dd-9a25-c63c9c0dac01"/><p id="1382e7d3-01e5-80ff-aa4e-fb4bc3596bc4" class=""><strong>Predictive Distribution</strong></p><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="1382e7d3-01e5-8065-84f4-c0607838b084"><div style="font-size:1.5em"><img class="icon" src="https://www.notion.so/icons/exclamation-mark-double_gray.svg"/></div><div style="width:100%"><p id="76900588-d6c1-48d7-8224-ee0c2be87811" class="">All you have to know is these expression for mean and variance. </p></div></figure><p id="1382e7d3-01e5-8079-b77c-e2df1660955f" class="">For a new test point, all you have to do is directly compute the mean &amp; variance. </p><figure id="1382e7d3-01e5-80fb-894f-f17967e49b69" class="image"><a href="L9b/IMG_DE853B2E1D2A-1.jpeg"><img style="width:288px" src="L9b/IMG_DE853B2E1D2A-1.jpeg"/></a></figure><figure class="block-color-default_background callout" style="white-space:pre-wrap;display:flex" id="1382e7d3-01e5-80f3-806c-cc7b204a82bc"><div style="font-size:1.5em"><img class="icon" src="https://www.notion.so/icons/exclamation-mark-double_gray.svg"/></div><div style="width:100%"><p id="479c5f2f-ce45-4b02-a9b3-40fef11804b8" class=""><strong>To compute the function value at any new point, </strong><link rel="stylesheet" type="text/css" href="notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">t_{n+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8234em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span></p><ol type="1" id="12d2e7d3-01e5-8090-9fe8-dba44220adfd" class="numbered-list" start="1"><li>Augment <link rel="stylesheet" type="text/css" href="notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span></span><span>﻿</span></span></li></ol><ol type="1" id="12d2e7d3-01e5-8071-b349-d66850905673" class="numbered-list" start="2"><li>Compute the mean and variance of the predictive distribution at the test point using <link rel="stylesheet" type="text/css" href="notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mrow><mi>N</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">C_{N+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8917em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span></li></ol></div></figure><figure class="block-color-default_background callout" style="white-space:pre-wrap;display:flex" id="1382e7d3-01e5-8030-8b4f-e4903fbb809c"><div style="font-size:1.5em"><img class="icon" src="https://www.notion.so/icons/exclamation-mark-double_gray.svg"/></div><div style="width:100%"><p id="385be3de-ec7b-4b4a-acd2-b37b0882d920" class=""><strong>Note — The Kernel Function </strong></p><hr id="1382e7d3-01e5-806d-84f7-e6e377488683"/><p id="1382e7d3-01e5-8000-905e-e890e484bb33" class="">The kernel function defines similarity (ie: it defines the properties covariance matrix <link rel="stylesheet" type="text/css" href="notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span></span><span>﻿</span></span>, which controls how function values are related to each other).</p><p id="1382e7d3-01e5-80da-94b5-ef4f87b7dd87" class="">For points that are similar, the function values will be strongly correlated. </p><p id="1382e7d3-01e5-8078-9400-cfb33e7a7903" class="">How we define “similar” depends on our choice of kernel function. </p><p id="1382e7d3-01e5-80cd-bf3f-e65fa41a6843" class="">The <strong>kernel </strong>does not need to be gaussian, BUT it must be a <strong>mercer kernel</strong> (positive semidefinite) to ensure <link rel="stylesheet" type="text/css" href="notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span></span><span>﻿</span></span> is</p><figure id="12d2e7d3-01e5-800a-96c9-ca198b64e7da" class="image" style="text-align:center"><a href="L9b/Screenshot_2024-10-28_at_19.15.20.png"><img style="width:432px" src="L9b/Screenshot_2024-10-28_at_19.15.20.png"/></a><figcaption>a) Gaussian kernel, b) exponential kernel </figcaption></figure></div></figure><figure class="block-color-default_background callout" style="white-space:pre-wrap;display:flex" id="1382e7d3-01e5-8079-a9bb-ef55532f1412"><div style="font-size:1.5em"><img class="icon" src="https://www.notion.so/icons/exclamation-mark-double_gray.svg"/></div><div style="width:100%"><p id="1382e7d3-01e5-806f-b286-d8e484cee527" class=""><strong>Similarity between linear regression &amp; gaussian process</strong></p><p id="1382e7d3-01e5-8021-aa1b-e3e885616eec" class="">When the kernel function is defined in terms of { basis functions }:   <div class="indented"><p id="bb63ed3b-2a24-4c5b-be39-588bfd81434b" class="">Predictive distributions of gaussian process &amp; linear regression are equal. </p><ul id="65b785b5-f68d-48cc-8d5b-64728569256c" class="bulleted-list"><li style="list-style-type:disc">linear regression — works from the parameter space perspective w’  </li></ul><ul id="5e5235d2-d270-4dbf-a09b-fbe409cd440e" class="bulleted-list"><li style="list-style-type:disc">gaussian process — works from the function space perspective w’ gaussian process. </li></ul></div></p><p id="1382e7d3-01e5-8047-a8bb-c097ffd60de2" class="">However the computation cost is different.<div class="indented"><figure id="fa0d987c-1aab-4043-b3aa-44da65591010" class="image"><a href="L9b/Screenshot_2024-10-28_at_19.18.40.png"><img style="width:528px" src="L9b/Screenshot_2024-10-28_at_19.18.40.png"/></a></figure></div></p></div></figure><p id="1382e7d3-01e5-80ba-ae4c-fd7460e8ceb0" class="">
</p><figure class="block-color-default_background callout" style="white-space:pre-wrap;display:flex" id="1382e7d3-01e5-80a8-be51-f5c7dd04fba5"><div style="font-size:1.5em"><img class="icon" src="https://www.notion.so/icons/exclamation-mark-double_gray.svg"/></div><div style="width:100%"><p id="aaf7cf5e-ac02-4047-a7d5-bdb93ee102a0" class=""><strong>What to know</strong></p><hr id="7b076cea-5f8a-4efa-ac8a-4bc379efce2f"/><ul id="d4a9ac80-fe40-4bac-b804-dbf3c0f583ca" class="toggle"><li><details open=""><summary>Apply Bayesian updating to determine the posterior distribution of parameters,from the likelihood and a given prior.</summary><p id="fa131ac9-1d47-49a7-941b-6fa6183bdde9" class="">
</p></details></li></ul><ul id="761e0064-d5d8-4e87-aa21-f85fdeba4efd" class="toggle"><li><details open=""><summary>Design suitable priors to reflect domain knowledge and serve as a form of regularization.</summary></details></li></ul><ul id="a4b61774-4964-4cd3-a26d-c4ff187252cf" class="toggle"><li><details open=""><summary>Use MAP to incorporate priors on the weights in regression in a data-scarce applications involving domain knowledge.</summary></details></li></ul><ul id="748442c0-4893-4b6d-92f2-88cb2dbe7860" class="toggle"><li><details open=""><summary>Interpret ridge regression as a imposing a prior on the distribution of weights.</summary></details></li></ul><ul id="9df609ee-d417-4187-9dc2-6a9c44641a2f" class="toggle"><li><details open=""><summary>Given the expression for the mean and covariance of the predictive distribution, and a particular kernel function, compute and/or sketch the prediction of the GP solution for a test input.</summary></details></li></ul><ul id="96ef5bb2-ee17-4181-ae5c-b61eedc4b591" class="toggle"><li><details open=""><summary>Select and defend the choice of using either Gaussian Process regression or Bayesian Linear Regression, taking into account the tradeoff between computational complexity and flexibility of the model.</summary><p id="43eb44d3-a5a1-4518-89d6-382db5f54ad1" class="">
</p></details></li></ul></div></figure></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>