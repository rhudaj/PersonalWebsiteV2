<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>L16 — Attention Mechanisms</title><link rel="stylesheet" type="text/css" href="../../notion_styles.css"/></head><body><article id="14c2e7d3-01e5-8018-807a-f71e4fdd4288" class="page sans"><header><div class="page-header-icon undefined"><img class="icon" src="https://www.notion.so/icons/document_blue.svg"/></div><h1 class="page-title">L16 — Attention Mechanisms</h1><p class="page-description"></p></header><div class="page-body"><p id="14c2e7d3-01e5-801d-819d-dc3217a93add" class=""><strong>topic: </strong>attention mechanisms </p><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="1602e7d3-01e5-805d-9e47-fb1601db49c3"><div style="font-size:1.5em"><img class="icon" src="https://www.notion.so/icons/exclamation-mark_gray.svg"/></div><div style="width:100%"><p id="21543e9e-a6b0-4414-baa1-a4f116e13edd" class=""><strong>L16 — Todo</strong></p><hr id="1602e7d3-01e5-80b9-b253-e4b44b2deb1b"/><ul id="1602e7d3-01e5-80af-a8be-d189366827e3" class="toggle"><li><details open=""><summary>Explain the operation of the encoder/decoder architecture of an RNN.</summary><ul id="1602e7d3-01e5-8005-8113-cd8a330b48a1" class="bulleted-list"><li style="list-style-type:disc">The encoder/decoder architecture is often used in sequence-to-sequence tasks like <strong>language translation</strong>.</li></ul><ul id="1602e7d3-01e5-807e-8671-ea19237326f3" class="bulleted-list"><li style="list-style-type:disc">The <strong>encoder</strong>, typically an RNN, processes the input sequence (e.g., a sentence in English) step-by-step.<ul id="3587e254-710d-4310-b977-0e53526995e7" class="bulleted-list"><li style="list-style-type:circle">It reads the input sequence and transforms it into a fixed-size <strong>context vector</strong>, which aims to capture the essence of the entire input.</li></ul><ul id="fc443fa5-dd66-481d-b837-52c7d4359ca9" class="bulleted-list"><li style="list-style-type:circle">The context vector is a representation of the input sequence that is meant to contain all the information necessary to produce the output sequence.</li></ul></li></ul><ul id="1602e7d3-01e5-80af-90a9-f6e68ecc2065" class="bulleted-list"><li style="list-style-type:disc">The <strong>decoder</strong>, also often an RNN, takes the context vector produced by the encoder as its initial input.<ul id="8b1bb98a-b140-4f94-89d5-d6006fe20dda" class="bulleted-list"><li style="list-style-type:circle">It then generates the output sequence (e.g., the translated sentence in French) step-by-step.</li></ul><ul id="223bcc41-e5e2-4548-8aa3-8a4e0203bac5" class="bulleted-list"><li style="list-style-type:circle">At each time step, the decoder uses the previous hidden state and the context vector to generate the next token in the output sequence.</li></ul></li></ul><ul id="1602e7d3-01e5-806e-ae25-f3de6eb7d0f6" class="bulleted-list"><li style="list-style-type:disc"><strong>Weakness</strong>: The encoder/decoder approach relies on the fixed-size vector to encode all the necessary infor from the input sequence. This can be a <strong>bottleneck</strong>, especially for long sequences, and can result in information loss.</li></ul></details></li></ul><ul id="1602e7d3-01e5-80eb-b02b-fe23c86e8498" class="toggle"><li><details open=""><summary>Describe the challenges of dealing with sequential data using RNNs, and the key<br/>advantages offered by attention as compared to RNNs.<br/></summary><ul id="1602e7d3-01e5-8041-a7f9-c376cb9ce3e0" class="bulleted-list"><li style="list-style-type:disc"><strong>Challenges of RNNs:</strong><ul id="78ce7b5d-72d8-4bcd-b95b-2de19747c80c" class="bulleted-list"><li style="list-style-type:circle"><strong>Information Bottleneck</strong>: RNNs, particularly in the encoder-decoder architecture, must compress all the information from the input sequence into a fixed-size context vector, which can lead to loss of information, especially in long sequences.</li></ul><ul id="55cb3ff3-d9e6-435a-88a5-4d043417f10f" class="bulleted-list"><li style="list-style-type:circle"><strong>Sequential Processing</strong>: RNNs process data sequentially, one step at a time, which limits their ability to parallelize computations. This makes them slower to train, and less efficient for utilizing the processing power of GPUs.</li></ul></li></ul><ul id="1602e7d3-01e5-80fd-88a8-f044aeaea00e" class="bulleted-list"><li style="list-style-type:disc"><strong>Advantages of Attention Mechanisms</strong>:<ul id="2a1b2ce9-46fe-4fad-8d21-a8063f47a461" class="bulleted-list"><li style="list-style-type:circle"><strong>Contextual Understanding</strong>: model can focus on different parts of the input sequence when generating the output, instead of relying on the fixed-size context vector. <ul id="1602e7d3-01e5-80a2-95db-ee8f46a861a7" class="bulleted-list"><li style="list-style-type:square">Useful in language translation where different parts of the input sentence are relevant for different parts of the translated output.</li></ul></li></ul><ul id="9c8cce90-b268-426e-a2bb-7849c3de2792" class="bulleted-list"><li style="list-style-type:circle"><strong>Direct Access to Input</strong>: Attention mechanisms eliminate the need for the &quot;middle-man&quot; (the fixed-length context vector) and allow the decoder to directly access the hidden states of the encoder at each time step. This gives the decoder context for each position in the input sequence and lets the decoder choose which parts of the input sequence to give attention.</li></ul><ul id="ca1eed91-2177-498d-8bba-21271fdbe12d" class="bulleted-list"><li style="list-style-type:circle"><strong>Parallelization</strong>: Attention mechanisms, particularly self-attention, can be computed in parallel, unlike RNNs, which process sequentially. This makes training faster and allows for better use of GPUs.</li></ul><ul id="51b6ed1a-d7d4-4341-9b45-89da30ab5e7d" class="bulleted-list"><li style="list-style-type:circle"><strong>Long-Range Dependencies</strong>: Attention mechanisms can capture both long-range and fine-grained relationships between elements in a sequence, which is useful in understanding context and meaning.</li></ul></li></ul></details></li></ul><ul id="1602e7d3-01e5-80cc-9f73-e148138d6fb2" class="toggle"><li><details open=""><summary>Compute attention for a short sequence given a queries, keys, and values.</summary><ul id="1602e7d3-01e5-80e1-9778-c3735b0eba38" class="bulleted-list"><li style="list-style-type:disc">Attention is calculated by comparing a <strong>query</strong> with a set of <strong>keys</strong> to generate <strong>weights</strong>, which are then used to combine the corresponding <strong>values</strong>.<ul id="b381b9b8-a813-4cbb-b9dc-86413e9755e2" class="bulleted-list"><li style="list-style-type:circle">The keys and values are part of the input.</li></ul><ul id="41d5bb69-f941-4c45-8315-413d55e2a09d" class="bulleted-list"><li style="list-style-type:circle">The query represents what the model is currently looking for.</li></ul></li></ul><ul id="1602e7d3-01e5-8013-b1df-d186c096f0e4" class="bulleted-list"><li style="list-style-type:disc"><strong>Steps for calculating attention:</strong><ol type="1" id="edd5a6d1-ad0e-4554-9041-3260d845fc77" class="numbered-list" start="1"><li><strong>Similarity Calculation</strong>: Compute the similarity between the query and each key. This can be done using a variety of methods, including dot products or learned weight matrices. These similarity scores are also called alignment scores.</li></ol><ol type="1" id="5b938afe-5ced-401a-b3de-414167b78538" class="numbered-list" start="2"><li><strong>Weight Calculation</strong>: Normalize the similarity scores using a <strong>softmax</strong> function to obtain the attention weights. These weights represent the importance of each key-value pair to the query.</li></ol><ol type="1" id="60b20596-2f96-4281-93fd-e126791149e8" class="numbered-list" start="3"><li><strong>Weighted Summation</strong>: Multiply each value by its corresponding attention weight and sum these weighted values to obtain a <strong>context vector</strong>, which is the output of the attention mechanism. This context vector is a weighted sum of the values, with the weights determined by the similarity between the query and keys.</li></ol></li></ul><ul id="1602e7d3-01e5-8036-a652-eb74b7a2b173" class="bulleted-list"><li style="list-style-type:disc">In essence, the attention mechanism allows the model to pay more attention to the most relevant parts of the input, based on the query.</li></ul></details></li></ul><ul id="1602e7d3-01e5-8012-9863-f3d30891cfeb" class="toggle"><li><details open=""><summary>Differentiate between attention, self-attention, masked attention, and multi-head<br/>attention.<br/></summary><ul id="1602e7d3-01e5-80b0-8c8d-d068855b008f" class="bulleted-list"><li style="list-style-type:disc"><strong>Attention</strong>: mechanism where a model focuses on relevant parts of an input when processing data. This often involves queries, keys, and values to generate weights that determine which parts of the input should be prioritized.</li></ul><ul id="1602e7d3-01e5-807e-8555-d85a1b38ec70" class="bulleted-list"><li style="list-style-type:disc"><strong>Self-Attention</strong>: input sequence attends to itself. In this model each token can attend to every other token in the same sequence to understand the relationship between the tokens.<ul id="d2caa5e5-0771-44c4-a14b-4831654ea598" class="bulleted-list"><li style="list-style-type:circle">Each input token generates its own query, key, and value, and these are used to compute the attention weights.</li></ul><ul id="dc3ca96a-7607-4611-be82-f9ffe8e20e1a" class="bulleted-list"><li style="list-style-type:circle">Self-attention allows for the parallel processing of all tokens in the sequence, which is an advantage over RNNs.</li></ul></li></ul><ul id="1602e7d3-01e5-80f5-8b66-f56195cf6c5b" class="bulleted-list"><li style="list-style-type:disc"><strong>Masked Attention</strong>: This is a variant of self-attention used in the decoder part of a transformer. It restricts the model from attending to future tokens in the sequence during training, as the future tokens would not be available during inference.<ul id="b340d80e-7d41-4e22-add5-7e00cf17e13b" class="bulleted-list"><li style="list-style-type:circle">This is necessary to prevent the model from &quot;cheating&quot; by using future context to predict current tokens.</li></ul></li></ul><ul id="1602e7d3-01e5-80da-9f74-ddeb19af7bf8" class="bulleted-list"><li style="list-style-type:disc"><strong>Multi-Head Attention</strong>: This involves using multiple attention mechanisms (called heads) in parallel.<ul id="952e6bdc-f86d-47e1-80c1-bc1564545041" class="bulleted-list"><li style="list-style-type:circle">Each head has its own set of learned weights and is designed to capture different types of relationships in the data. The results of the heads are then combined to produce a comprehensive representation of the input.</li></ul><ul id="973dc846-90cf-44d6-8e6c-7021f0f4b022" class="bulleted-list"><li style="list-style-type:circle">Multi-head attention improves the model&#x27;s ability to capture subtle and long-range dependencies, and different aspects of similarity.</li></ul></li></ul></details></li></ul><ul id="1602e7d3-01e5-8048-9d95-f4c11b3c787d" class="toggle"><li><details open=""><summary>Label the key components of the transformer architecture.</summary><ul id="1602e7d3-01e5-8016-8b68-ed4d1265f5c9" class="bulleted-list"><li style="list-style-type:disc">The Transformer architecture is based on the concept of attention, with the addition of positional encoding and multi-head attention.</li></ul><ul id="1602e7d3-01e5-8046-a297-eaa426449aff" class="bulleted-list"><li style="list-style-type:disc"><strong>Encoder</strong>: The encoder takes the input sequence and generates an attention-based representation. It is composed of multiple stacked layers of the following:<ul id="1083fb71-9e05-4a7f-b7bc-fb95ef0ec7c7" class="bulleted-list"><li style="list-style-type:circle"><strong>Multi-Head Self-Attention Layer</strong>: Each input token attends to all other tokens in the sequence.</li></ul><ul id="c9d6fc66-0925-415b-9e63-e565955310d6" class="bulleted-list"><li style="list-style-type:circle"><strong>Feedforward Layer</strong>: A point-wise feedforward layer that applies the same transformation to each sequence position independently.</li></ul><ul id="7d09bc89-9120-4bdf-ab74-cc684792a358" class="bulleted-list"><li style="list-style-type:circle"><strong>Add &amp; Norm Layer</strong>: Each sub-layer uses an add and norm operation to stabilize the network.</li></ul></li></ul><ul id="1602e7d3-01e5-8076-8d2f-c9f36a49e6ee" class="bulleted-list"><li style="list-style-type:disc"><strong>Decoder</strong>: The decoder generates the output sequence using information from the encoder and its own previous outputs. It is composed of multiple stacked layers of the following:<ul id="2ff9f62c-b320-4ed8-99f7-0f3dd3f8a639" class="bulleted-list"><li style="list-style-type:circle"><strong>Masked Multi-Head Self-Attention Layer</strong>: Each position only attends to earlier positions in the sequence.</li></ul></li></ul><ul id="1602e7d3-01e5-80ef-96a9-f2e46ac0f6ed" class="bulleted-list"><li style="list-style-type:disc"><strong>Multi-Head Attention Layer</strong>: Uses the encoder outputs as keys and values and the decoder outputs as queries.</li></ul><ul id="1602e7d3-01e5-80fa-8c30-f5e2c4455536" class="bulleted-list"><li style="list-style-type:disc"><strong>Feedforward Layer</strong>: A point-wise feedforward layer that applies the same transformation to each sequence position independently.</li></ul><ul id="1602e7d3-01e5-80e5-a030-d3f761b5cbc6" class="bulleted-list"><li style="list-style-type:disc"><strong>Add &amp; Norm Layer</strong>: Each sub-layer uses an add and norm operation to stabilize the network.</li></ul><ul id="1602e7d3-01e5-80e6-9721-c2ba9bcf1a59" class="bulleted-list"><li style="list-style-type:disc"><strong>Positional Encoding</strong>: Since attention mechanisms don&#x27;t account for the order of tokens, positional encoding is added to the input embeddings to give the model information about token position.<ul id="4d884a52-9a00-4a1b-b605-79aaf96d1c48" class="bulleted-list"><li style="list-style-type:circle">This is added to the embedding matrix and uses a unique “fingerprint” for each position in the input sequence.</li></ul></li></ul></details></li></ul><ul id="1602e7d3-01e5-808e-8c27-c11766ab09d8" class="toggle"><li><details open=""><summary>Describe the generative pre-training paradigm as used in GPT-1 with reference to<br/>the appropriate objectives.<br/></summary><ul id="1602e7d3-01e5-8013-894c-e75ce846eb7e" class="bulleted-list"><li style="list-style-type:disc">The Generative Pre-Training Transformer (GPT) uses a two-phase training approach: <strong>unsupervised pre-training</strong> and <strong>supervised fine-tuning</strong>.<ul id="c295e5ed-492c-4ff1-8df4-73ba39c85114" class="bulleted-list"><li style="list-style-type:circle"><strong>Unsupervised Pre-Training</strong>: The model is trained on a large corpus of raw text. The objective is to learn to predict the next word in a sequence. This allows the model to learn a general representation of language without being tied to any specific task.</li></ul><ul id="619e4f4c-b5c2-4ac4-8a3f-5fd4ef096dde" class="bulleted-list"><li style="list-style-type:circle"><strong>Supervised Fine-Tuning</strong>: After pre-training, the model is fine-tuned on a specific task (such as classification or summarization). The model learns to predict labels or other task-specific outputs while retaining the general knowledge learned during the pre-training phase.</li></ul></li></ul><ul id="1602e7d3-01e5-8088-9782-fa510bc186f2" class="bulleted-list"><li style="list-style-type:disc"><strong>Balanced Training</strong>: The fine-tuning is balanced with the pre-training, to ensure the model retains the generalized understanding of language it developed during pre-training.</li></ul><ul id="1602e7d3-01e5-80bb-b75f-c53323606755" class="bulleted-list"><li style="list-style-type:disc"><strong>In-Context Learning</strong>: A key feature of GPT models is their ability to adapt to new tasks based on a prompt without requiring explicit retraining, which allows for &quot;in context learning&quot;.</li></ul></details></li></ul><ul id="1602e7d3-01e5-809b-89f2-e0e4a8b7af35" class="toggle"><li><details open=""><summary>Recall significant applications of the transformer in language modeling.</summary><ul id="1602e7d3-01e5-8006-afc6-ed70b08d61cb" class="bulleted-list"><li style="list-style-type:disc">language translation and generation, as well as their ability to parallelize computations.</li></ul><ul id="1602e7d3-01e5-8020-b5e9-c4a8b7d07534" class="bulleted-list"><li style="list-style-type:disc">GPT models for generative pre-training, indicating the utility of transformers for general language understanding and generation tasks.</li></ul></details></li></ul><ul id="1602e7d3-01e5-8055-b04f-d4e842fca393" class="toggle"><li><details open=""><summary>Discriminate between fine-tuning and zero-, one-, and few-shot generation.</summary><ul id="1602e7d3-01e5-80ed-8cd7-eb47bd7126fb" class="bulleted-list"><li style="list-style-type:disc"><strong>Fine-tuning</strong>: This involves taking a pre-trained model and training it further on a specific task with labeled data. The model is updated via backpropagation. This approach leverages the general knowledge acquired during pre-training and adapts it to the new task.</li></ul><ul id="1602e7d3-01e5-805c-8378-d01c7293e261" class="bulleted-list"><li style="list-style-type:disc"><strong>Zero-shot Generation</strong>: The model is directly prompted to complete a task without any additional training or task-specific examples. The model attempts to perform the task based on its pre-existing general knowledge of language.</li></ul><ul id="1602e7d3-01e5-8075-819a-f0c2abb2b246" class="bulleted-list"><li style="list-style-type:disc"><strong>One-shot Generation</strong>: The model is provided with a single example of the desired task, then prompted to complete a similar task, often without any gradient updates. The model leverages the context provided in the prompt to complete the new task.</li></ul><ul id="1602e7d3-01e5-8082-8016-ed8df38510e1" class="bulleted-list"><li style="list-style-type:disc"><strong>Few-shot Generation</strong>: The model is given a few examples of the desired task in the prompt, which guides its completion of similar tasks. This method does not retrain the model on the examples but rather leverages the context provided in the prompt.</li></ul></details></li></ul><ul id="1602e7d3-01e5-80c3-ab31-cda73140eb67" class="toggle"><li><details open=""><summary>Defend the scaleability issues of transformer-based attention models and .</summary><ul id="1602e7d3-01e5-8081-9b84-fc9e0c216da3" class="bulleted-list"><li style="list-style-type:disc"><strong>Computational Cost</strong>: One of the main drawbacks of transformer-based models, especially self-attention, is the high computational cost, especially since each position attends to every other position. This can become very expensive for long sequences, and is more computationally expensive than an RNN that only attends to the previous token.</li></ul><ul id="1602e7d3-01e5-805a-964d-d2cc638ad39e" class="bulleted-list"><li style="list-style-type:disc"><strong>Memory Usage</strong>: Due to the need to compute attention scores for every pair of tokens, the memory requirements can increase rapidly with longer sequences.</li></ul><ul id="1602e7d3-01e5-8015-a952-cc8e6c5fe764" class="bulleted-list"><li style="list-style-type:disc"><strong>Quadratic Complexity</strong>: Self-attention has quadratic complexity with respect to sequence length, meaning that as the length of the sequence increases the computational cost grows exponentially. This can make it difficult to scale transformers to very long input sequences.</li></ul></details></li></ul></div></figure><hr id="15f2e7d3-01e5-802e-8538-f29bb418d2c6"/><hr id="1602e7d3-01e5-80dc-924b-d63859c111f1"/><p id="1602e7d3-01e5-8001-a3e0-e8e4acf914d9" class="">
</p><h3 id="15f2e7d3-01e5-80d8-bbbe-e9da28ffdaa4" class="">Attention (Idea) </h3><p id="14c2e7d3-01e5-8092-954e-f92c226c4001" class="">Until now, we’ve been training a <em>fixed-size</em> set of weights. Once trained, the weights themselves are also fixed (the same for any input): </p><ul id="14c2e7d3-01e5-8034-a844-ee3d0176ff73" class="toggle"><li><details open=""><summary><mark class="highlight-purple">Example (N.N)</mark></summary><figure id="14c2e7d3-01e5-80f5-8b61-de969af25d74" class="image" style="text-align:center"><a href="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-27_at_22.25.39.png"><img style="width:480px" src="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-27_at_22.25.39.png"/></a></figure></details></li></ul><p id="15f2e7d3-01e5-8043-8a69-fbdf0ed2090c" class="">What if the weights <em>depended on the input </em>? Then the model depends on the current situation <em>(”redirects </em><em><strong>attention</strong></em><em>” depending on the context). </em></p><ul id="15f2e7d3-01e5-80d1-b367-fdafadaf918f" class="toggle"><li><details open=""><summary><mark class="highlight-purple">Example (N.N)</mark></summary><figure id="14c2e7d3-01e5-800b-9944-f00be15c04a4" class="image"><a href="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-27_at_22.26.00.png"><img style="width:528px" src="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-27_at_22.26.00.png"/></a><figcaption>The weights are a function of the CURRENT input.</figcaption></figure></details></li></ul><p id="14c2e7d3-01e5-8041-9f59-ee99c0e59329" class=""><strong>Attention </strong>is useful for many tasks</p><ul id="15f2e7d3-01e5-80e7-b8d9-e9c360c72ce1" class="toggle"><li><details open=""><summary><mark class="highlight-purple">Example ( language translation )</mark></summary><ul id="15f2e7d3-01e5-8058-88cf-df7368337006" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-default">The context needed to translate something varies. You don’t need any to translate a work like “lunedi” or “mercredi” but you need more for a phrase like “Bonjour je m’appelle”. </mark></li></ul></details></li></ul><p id="14c2e7d3-01e5-8037-a54e-e6b39373e64a" class="">We don’t need to use <strong>RNN</strong>’s to get attention.</p><hr id="14c2e7d3-01e5-8062-81cf-eb0c8c15c34e"/><h2 id="14c2e7d3-01e5-8049-9419-f25adcf294a7" class=""><strong>Attention — for Sequential Tasks </strong></h2><p id="14c2e7d3-01e5-8058-8c78-c14506985fa8" class=""><strong>Sequence-to-Sequence Models have an information Bottleneck</strong></p><ul id="14c2e7d3-01e5-8078-a951-f17e1d42a145" class="bulleted-list"><li style="list-style-type:disc">One solution is <strong>encoder/decoder </strong>which takes advantage of RNNs. For the task of language translation, an encoder (RNN) will take the english language sequence as input and output a fixed sized context vector. This is then fed into a decoder (RNN) which determines a french sequence. <ul id="14d2e7d3-01e5-80e6-abc2-df28e570738f" class="bulleted-list"><li style="list-style-type:circle"><strong>Weakness: </strong>relies on this fixed sized vector to encode all the necessary information from the input. As a result it does not work well for <strong>long sentences</strong>. </li></ul></li></ul><figure id="14c2e7d3-01e5-8031-a537-d3916213fe6e" class="image" style="text-align:center"><a href="L16%20%E2%80%94%20Attention%20Mechanisms/6489a0d5-624a-45fa-9fe7-02b43cfaf778.png"><img style="width:336px" src="L16%20%E2%80%94%20Attention%20Mechanisms/6489a0d5-624a-45fa-9fe7-02b43cfaf778.png"/></a></figure><hr id="14c2e7d3-01e5-802a-ab9f-f05db95b40f6"/><p id="14c2e7d3-01e5-80d4-9438-e59fa8971fe3" class=""><strong>Adding a Context Vector to fix the Bottleneck</strong></p><ul id="14c2e7d3-01e5-80c9-b93b-db15e4ee476c" class="bulleted-list"><li style="list-style-type:disc">Idea: get rid of the “middle-man” (fixed-length state) and let the decoder and encoder “talk to each-other”</li></ul><figure id="14c2e7d3-01e5-8053-9a7f-d8e1a225b51a" class="image" style="text-align:center"><a href="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-27_at_22.45.08.png"><img style="width:288px" src="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-27_at_22.45.08.png"/></a></figure><p id="14c2e7d3-01e5-8080-adaf-e8ce38ec1b87" class="">
</p><ul id="14c2e7d3-01e5-806a-8c97-da2261744b1f" class="bulleted-list"><li style="list-style-type:disc">The decoder can access the hidden state of the encoder at each time step <em><mark class="highlight-gray">(e.g. context for a specific position in the input sequence)</mark></em>. </li></ul><ul id="14c2e7d3-01e5-80a0-a20d-dec37afbfb82" class="bulleted-list"><li style="list-style-type:disc">The decoder can now choose which of these states to give <strong>attention</strong>.</li></ul><ul id="14c2e7d3-01e5-80f1-b98a-f0c08dab27ed" class="bulleted-list"><li style="list-style-type:disc"><strong>Attention mechanism </strong><ul id="14d2e7d3-01e5-80e0-93b2-e552cb74f68a" class="bulleted-list"><li style="list-style-type:circle">Has {weights}, |weights| = |hidden states|, which change <strong>dynamically</strong> based on the state of the decoder. </li></ul></li></ul><ul id="14d2e7d3-01e5-80bd-92f6-e8765f825ed2" class="bulleted-list"><li style="list-style-type:disc"><strong>Aggregation</strong><ul id="14d2e7d3-01e5-8024-9716-ff50cf2cd39b" class="bulleted-list"><li style="list-style-type:circle">Combines the weighted attention states ⇒ into a <strong>context vector</strong></li></ul></li></ul><ul id="14d2e7d3-01e5-8053-8e6f-e693104cdb0f" class="bulleted-list"><li style="list-style-type:disc">The decoder is fed this this <strong>context vector </strong>which is specifically tailored for the current time-step/input. </li></ul><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0"><strong>Bateneau Mechanism (specific Implementation)</strong></summary><div class="indented"><ul id="14c2e7d3-01e5-8072-9e6c-ce147c818702" class="bulleted-list"><li style="list-style-type:disc">Each part of the input sequence is given some an <strong>alignment score </strong>which is determined based on the encoders hidden states and the previous output: </li></ul><figure id="14d2e7d3-01e5-80ce-8b3a-f556f733cc7d" class="equation"><link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>e</mi><mrow><mi>t</mi><mo separator="true">,</mo><mi>i</mi></mrow></msub><mo>=</mo><mi>a</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>h</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">e_{t,i}=a(s_{t-1}, h_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">a</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></div></figure><ul id="14d2e7d3-01e5-8060-8047-f6610ec1b26a" class="bulleted-list"><li style="list-style-type:disc">Use the score to determine the <strong>attention weight  — </strong>measure of how important  this part of the input is for determining the output. </li></ul><figure id="14d2e7d3-01e5-8087-a314-ef391af131cc" class="equation"><link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>α</mi><mrow><mi>t</mi><mo separator="true">,</mo><mi>i</mi></mrow></msub><mo>=</mo><mtext>Sigmoid</mtext><mo stretchy="false">(</mo><msub><mi>e</mi><mrow><mi>t</mi><mo separator="true">,</mo><mi>i</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\alpha_{t,i}=\text{Sigmoid}(e_{t,i})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord text"><span class="mord">Sigmoid</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></div></figure><ul id="14d2e7d3-01e5-804b-9365-f69e87e7c995" class="bulleted-list"><li style="list-style-type:disc">Finally, these are combined to determine the <strong>context vector</strong> for the decoder</li></ul><figure id="14d2e7d3-01e5-80be-a348-e228d80fdd1d" class="equation"><link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>c</mi><mi>t</mi></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><msub><mi>α</mi><mrow><mi>t</mi><mo separator="true">,</mo><mi>i</mi></mrow></msub><msub><mi>h</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">c_t=\sum_{i=1}^T \alpha_{t,i}h_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.106em;vertical-align:-1.2777em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></div></figure><ul id="14c2e7d3-01e5-80c1-867e-f6fe34702392" class="toggle"><li><details open=""><summary><mark class="highlight-purple"><strong>Example</strong></mark></summary><figure id="14c2e7d3-01e5-8098-a5d2-f08e130907d3" class="image" style="text-align:center"><a href="L16%20%E2%80%94%20Attention%20Mechanisms/26256b4e-fb44-46c1-a0a7-fdca5e03789c.png"><img style="width:240px" src="L16%20%E2%80%94%20Attention%20Mechanisms/26256b4e-fb44-46c1-a0a7-fdca5e03789c.png"/></a><figcaption>Brighr colors ⇒ high attention weights</figcaption></figure><p id="14c2e7d3-01e5-8048-8c25-fa63b525a771" class="">Each row shows the <strong>context vector </strong>shows the  current context vector (start from <link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">t=0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span></span><span>﻿</span></span> at the top. The color indicates the <strong>weight </strong>that the part of the input sequence (english word) has in the context vector. For example, “destruction<strong>”. </strong>(french) translation really only depends on “destruction” (english). You don’t need much more context. So for this case, its pretty much the same as the encoders hidden state. But not for “que” which is a more generic word. It has a greater context required. </p></details></li></ul></div></details><h3 id="14c2e7d3-01e5-8053-885b-ee388f0750d1" class=""><strong>Attention Mechanism In General </strong></h3><p id="14d2e7d3-01e5-8019-949d-dcdc0caeb616" class="">View it as a database, <link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span></span></span></span></span><span>﻿</span></span> of <link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span></span><span>﻿</span></span> (key, value) pairs</p><ul id="14d2e7d3-01e5-8096-8fb0-c0254cd8b062" class="bulleted-list"><li style="list-style-type:disc"><strong>key — </strong>part of the input</li></ul><ul id="14d2e7d3-01e5-802e-987b-dbb2dd6c7a93" class="bulleted-list"><li style="list-style-type:disc"><strong>value — </strong>representation of “info” associated with a key (context)</li></ul><ul id="14d2e7d3-01e5-805f-930a-ea89517991b4" class="bulleted-list"><li style="list-style-type:disc">The <strong>weights </strong>compare similarity of <strong>keys</strong> to a <strong>query </strong></li></ul><p id="14d2e7d3-01e5-8096-b813-e49b5fa29557" class="">The <strong>attention</strong> over <link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span></span></span></span></span><span>﻿</span></span> for some <strong>query</strong> is: </p><figure id="14d2e7d3-01e5-8030-b9ee-f4c968abf3d1" class="equation"><link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><mi mathvariant="bold">q</mi><mo separator="true">,</mo><mi>D</mi><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>α</mi><mo stretchy="false">(</mo><mi mathvariant="bold">q</mi><mo separator="true">,</mo><msub><mi mathvariant="bold">k</mi><mi>i</mi></msub><mo stretchy="false">)</mo><msub><mi mathvariant="bold">v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\text{Attention}(\bold q, D)= \sum_{i=1}^m \alpha (\bold q,\bold k_i) \bold v_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathbf">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.9291em;vertical-align:-1.2777em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6514em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mopen">(</span><span class="mord mathbf">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbf">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></div></figure><ul id="14d2e7d3-01e5-809d-ace0-ef68b0890048" class="bulleted-list"><li style="list-style-type:disc">Attention weights, <link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span></span><span>﻿</span></span>, are scalar</li></ul><figure id="14c2e7d3-01e5-8052-b880-ef83bde3e19e" class="image" style="text-align:center"><a href="L16%20%E2%80%94%20Attention%20Mechanisms/ea798aed-7de7-40a9-bd33-5e3bcd21201d.png"><img style="width:336px" src="L16%20%E2%80%94%20Attention%20Mechanisms/ea798aed-7de7-40a9-bd33-5e3bcd21201d.png"/></a></figure><ul id="14c2e7d3-01e5-8091-8652-ecee61949d85" class="bulleted-list"><li style="list-style-type:disc">We compare the query with each key, and generate weights for each associated value (a vector). Then we combine each of these weighted vectors into an output vector. </li></ul><ul id="14c2e7d3-01e5-8038-8734-fd537f7c756d" class="bulleted-list"><li style="list-style-type:disc"><strong>attention pooling </strong>— allows model to pay more attention to vectors associated with most relevant keys, based on the <strong>query</strong><ul id="14c2e7d3-01e5-8038-9204-f19642d00e71" class="bulleted-list"><li style="list-style-type:circle">If all but 1 weight are 0 ⇒ dictionary lookup</li></ul><ul id="14c2e7d3-01e5-809b-a053-ecc219c29bb6" class="bulleted-list"><li style="list-style-type:circle">If each weight is equal ⇒ average of all values. </li></ul></li></ul><ul id="14c2e7d3-01e5-8080-8046-d06eaef7f1ca" class="toggle"><li><details open=""><summary><strong>How to compute </strong><link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span></span><span>﻿</span></span><strong>’s</strong></summary><figure id="14c2e7d3-01e5-8049-af4e-de9c4a6aeb87" class="image" style="text-align:center"><a href="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-27_at_23.14.56.png"><img style="width:576px" src="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-27_at_23.14.56.png"/></a></figure><ul id="14c2e7d3-01e5-806c-b1e1-ca29f5916681" class="bulleted-list"><li style="list-style-type:disc">(batches of data) computing attention over multiple inputs simultaneously; if we have a collection of <link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span></span><span>﻿</span></span> queries, keys, keys (ie: d x n). The values can have whatever dimension. We apply softmax to attension weights to normalize them, to get a weightd</li></ul></details></li></ul><h1 id="15f2e7d3-01e5-8007-af24-c8412f2d6acb" class="">Attention — As Parallel Architecture </h1><ul id="14d2e7d3-01e5-80c7-962c-de31895012be" class="bulleted-list"><li style="list-style-type:disc">We introduced attention in RNNs for solving language problems (e.g. language translation) where we are trying to align some input sequence to an output sequence. </li></ul><ul id="15f2e7d3-01e5-802b-ac74-e7f911f6725f" class="bulleted-list"><li style="list-style-type:disc">These tasks are tough to <strong>parallelize </strong>because they process data <strong>sequentially</strong>. </li></ul><ul id="15f2e7d3-01e5-8056-8e1f-d9292c8dd3d8" class="bulleted-list"><li style="list-style-type:disc">What if we align a sequence <strong>with itself </strong>(i.e. relationships within a sequence)<p id="14c2e7d3-01e5-8020-b5d2-f5493f9720c5" class="">⇒ we look t<em>owards a </em><em><strong>feedforward attention model</strong></em>. </p></li></ul><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="14d2e7d3-01e5-806b-a4c3-c5935a60ed2f"><div style="font-size:1.5em"><img class="icon" src="https://www.notion.so/icons/exclamation-mark_gray.svg"/></div><div style="width:100%"><p id="8be6d951-ec63-4c58-bf06-49fe1bfe9f4f" class="">Parallel tasks leverage <strong>GPUs </strong>better</p></div></figure><h3 id="14c2e7d3-01e5-8078-9522-c6943455015d" class=""><strong>Self Attention</strong></h3><ul id="15f2e7d3-01e5-8020-908b-d1a0a9eb10dc" class="bulleted-list"><li style="list-style-type:disc">Computes attention between tokens, <em>across the whole sequence at once</em>. </li></ul><ul id="15f2e7d3-01e5-80e7-a066-ed47287a6dbc" class="bulleted-list"><li style="list-style-type:disc">This can be <strong>parallelized</strong></li></ul><ul id="15f2e7d3-01e5-80bd-942f-f186e9c11311" class="bulleted-list"><li style="list-style-type:disc">Every token can directly “attend” to every other token </li></ul><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="14d2e7d3-01e5-806c-b560-e5f5afe45b04"><div style="font-size:1.5em"><img class="icon" src="https://www.notion.so/icons/exclamation-mark_gray.svg"/></div><div style="width:100%"><p id="ced49974-1a53-412d-8dca-6d187d668170" class=""><strong>Self Attention Model</strong></p><hr id="15f2e7d3-01e5-8000-a7f1-faf1a55fb726"/><p id="14d2e7d3-01e5-80d9-b30d-f5acdbf5058f" class="">Each input token (<link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span>) can <strong>attend to itself </strong>(generate its <strong>own</strong> key, value, query)</p><ul id="14d2e7d3-01e5-8059-98fd-cdf3d5a3c462" class="bulleted-list"><li style="list-style-type:disc"><strong>value — </strong>representation of what the input is </li></ul><ul id="14d2e7d3-01e5-80a7-9b57-ee57073887ea" class="bulleted-list"><li style="list-style-type:disc"><strong>key — </strong>what it should focus on in the other inputs</li></ul><ul id="14d2e7d3-01e5-8059-afb8-e9dd0226bdb9" class="bulleted-list"><li style="list-style-type:disc"><strong>query — </strong>what its looking for</li></ul><hr id="14d2e7d3-01e5-8036-963d-cfeb4ef93317"/><p id="14d2e7d3-01e5-807f-aad5-e03c1fe050f3" class="">Self attention generates a sequence: </p><figure id="14d2e7d3-01e5-80ab-81ea-c0a2858cd32f" class="equation"><link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mtext>Attn</mtext><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">,</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mi>n</mi></msub><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">y_i=\text{Attn}(x_i, (x_1,x_1),\dots, (x_n,x_n))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Attn</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">))</span></span></span></span></span></div></figure><ul id="14d2e7d3-01e5-804d-9d3d-eeac00cb88ac" class="bulleted-list"><li style="list-style-type:disc"><link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> — <strong>query</strong></li></ul><ul id="14d2e7d3-01e5-8095-bc5c-c72c0bc25cce" class="bulleted-list"><li style="list-style-type:disc">Valid inputs <link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">x_1,\dots, x_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> — <strong>keys/values</strong></li></ul></div></figure><div id="15f2e7d3-01e5-8012-9cb4-c2f361a6040e" class="column-list"><div id="6f3ade61-488b-4593-a25c-e93191f3a994" style="width:25%" class="column"><figure id="14c2e7d3-01e5-804a-b01c-fb808ffd9e61" class="image"><a href="L16%20%E2%80%94%20Attention%20Mechanisms/486b4fa9-6b8c-48dd-9b7c-a83f3b40993a.png"><img style="width:144px" src="L16%20%E2%80%94%20Attention%20Mechanisms/486b4fa9-6b8c-48dd-9b7c-a83f3b40993a.png"/></a></figure></div><div id="170f3a8e-8521-4fd0-b073-8d0880f5997d" style="width:75%" class="column"><figure id="15f2e7d3-01e5-80ca-b144-ce872039878b" class="image"><a href="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-29_at_16.55.20.png"><img style="width:480px" src="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-29_at_16.55.20.png"/></a></figure></div></div><ol type="1" id="15f2e7d3-01e5-8051-b324-df04a8623aa7" class="numbered-list" start="1"><li>Convert each <link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> into (<link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span></span></span></span></span><span>﻿</span></span>, <link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span></span><span>﻿</span></span>, <link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span></span></span></span><span>﻿</span></span>) using separate <strong>learned matrices</strong></li></ol><ol type="1" id="14d2e7d3-01e5-8059-8750-f43590d2d2b6" class="numbered-list" start="2"><li>Compare each <link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>q</mi><mi>i</mi></msup></mrow><annotation encoding="application/x-tex">q^i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0191em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8247em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span>, <link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>k</mi><mi>j</mi></msup></mrow><annotation encoding="application/x-tex">k^{j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8247em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8247em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> ⇒ <strong>aligment scores </strong><link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ω</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\omega_{i,j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span><strong>: </strong><link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>×</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">Q \times K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span></span><span>﻿</span></span> ⇒ <link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ω</mi></mrow><annotation encoding="application/x-tex">\omega</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">ω</span></span></span></span></span><span>﻿</span></span><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="14d2e7d3-01e5-800a-b318-d8eeebef6992"><div style="font-size:1.5em"><img class="icon" src="https://www.notion.so/icons/exclamation-mark_gray.svg"/></div><div style="width:100%"><p id="c02a7bb7-c215-42d2-8a30-b990a4177195" class="">We compute attention scores across the whole sequence at once (in parallel, not sequentially). </p></div></figure></li></ol><ol type="1" id="15f2e7d3-01e5-8047-b345-d23b36769d75" class="numbered-list" start="3"><li>This is scaled.</li></ol><ol type="1" id="15f2e7d3-01e5-803a-a9bf-c7087fa9673a" class="numbered-list" start="4"><li>Masking (optional)</li></ol><ol type="1" id="15f2e7d3-01e5-8068-9632-e2deb99184a3" class="numbered-list" start="5"><li>Normalized (softmax) to produce <strong>attention weights, </strong><link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span></span><span>﻿</span></span><ul id="da14a9fa-2969-4b10-afab-cd0c6bc7d0f8" class="bulleted-list"><li style="list-style-type:disc">i.e. how much focus should be place on each value</li></ul></li></ol><ol type="1" id="15f2e7d3-01e5-8062-9544-c480358cca2b" class="numbered-list" start="6"><li>Finally, we multiply with the <link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></span><span>﻿</span></span> matrix ⇒ <strong>context aware representation</strong>, <link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span></span></span></span><span>﻿</span></span></li></ol><hr id="14d2e7d3-01e5-803e-8d0d-df9ba442394b"/><ul id="14d2e7d3-01e5-8007-9355-e07b07ec65ff" class="toggle"><li><details open=""><summary><mark class="highlight-purple"><strong>Example — Language Translation w’ Self Attention</strong></mark></summary><p id="14d2e7d3-01e5-80ef-8abd-cbcff353eb3e" class="">In french, every noun is M or F. To translate a sentence, we need to know if the noun to which “it” refers to is M or F. So “it” token needs to attend to those nouns.</p><p id="15f2e7d3-01e5-80c2-8463-c33ad4c1ec97" class="">Those key nouns have large <strong>attention weights</strong>. </p><figure id="14d2e7d3-01e5-80a1-a5d9-d758ff761356" class="image" style="text-align:center"><a href="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-29_at_17.01.18.png"><img style="width:384px" src="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-29_at_17.01.18.png"/></a></figure></details></li></ul><hr id="14d2e7d3-01e5-807a-94f5-de9909111e3d"/><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="15f2e7d3-01e5-8089-8935-c59aaf88450a"><div style="font-size:1.5em"><img class="icon" src="https://www.notion.so/icons/exclamation-mark_red.svg"/></div><div style="width:100%"><p id="503b985e-f87f-45c3-8347-4c172e1c22e2" class=""><strong>Problem</strong></p><p id="14d2e7d3-01e5-80e8-b489-e0f30dda8ca4" class="">Self attention doesn’t care about the order of the tokens. But, in real language, the order does matter. </p><p id="15f2e7d3-01e5-805c-948e-fcd9c264688e" class=""><em><mark class="highlight-gray"><strong>Solution: </strong></mark></em><em><mark class="highlight-gray">Positional Encoding Scheme </mark></em></p></div></figure><h3 id="14d2e7d3-01e5-802d-a9b5-e38f45e90371" class="">Positional Encoding Scheme </h3><p id="14d2e7d3-01e5-8036-bd46-c7bd1fc821c4" class="">Encode the positions into the inputs (word-embedding)</p><figure id="14d2e7d3-01e5-8054-9009-f2e849251ee0" class="image"><a href="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-29_at_17.06.02.png"><img style="width:336px" src="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-29_at_17.06.02.png"/></a></figure><ul id="14d2e7d3-01e5-803d-ba0d-fa5ccae0a4d9" class="bulleted-list"><li style="list-style-type:disc">Add a <strong>positional encoding matrix </strong><link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span></span><span>﻿</span></span><strong> </strong>to the embedding matrix <link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span></span><span>﻿</span></span><ul id="14d2e7d3-01e5-8024-b333-f0166cf3ec11" class="bulleted-list"><li style="list-style-type:circle"><link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{i,j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> = position <link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span></span><span>﻿</span></span> in input sequence, <link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span></span><span>﻿</span></span>th embedding dimension. <ul id="14d2e7d3-01e5-80d2-b10a-eaa2fc0edd0c" class="bulleted-list"><li style="list-style-type:square">Want each position to have a unique “fingerprint”</li></ul><ul id="14d2e7d3-01e5-800f-a02d-e0e708143154" class="bulleted-list"><li style="list-style-type:square">e.g. use sinusoids, with different frequencies for each dimension. </li></ul></li></ul></li></ul><ul id="14d2e7d3-01e5-803f-b73a-d3d917ce1073" class="toggle"><li><details open=""><summary><mark class="highlight-purple"><strong>Example</strong></mark></summary><figure id="14d2e7d3-01e5-80ab-80b2-f9b137ef2ba3" class="image" style="text-align:center"><a href="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-29_at_17.06.22.png"><img style="width:336px" src="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-29_at_17.06.22.png"/></a></figure><ul id="14d2e7d3-01e5-80e5-924c-e494c4ea250b" class="bulleted-list"><li style="list-style-type:disc">x-axis: position <link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span></span><span>﻿</span></span></li></ul><ul id="14d2e7d3-01e5-80d4-88bd-ef4efb683c90" class="bulleted-list"><li style="list-style-type:disc">y-axis: value of <link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{i,j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span>. Color for each <link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span></span><span>﻿</span></span> </li></ul><ul id="14d2e7d3-01e5-80a5-b9dc-edd8a6f0bcee" class="bulleted-list"><li style="list-style-type:disc">Bottom graph is <link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span></span><span>﻿</span></span> <ul id="14d2e7d3-01e5-80c0-b11a-c65fa48317bf" class="bulleted-list"><li style="list-style-type:circle">left most columns oscilate the fastest </li></ul></li></ul></details></li></ul><ul id="14d2e7d3-01e5-80fe-9362-de2b66ccdd58" class="toggle"><li><details open=""><summary><mark class="highlight-purple"><strong>Example — Compute </strong></mark><mark class="highlight-purple"><link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span></span><span>﻿</span></span></mark></summary><p id="14d2e7d3-01e5-8028-b0b8-df40c4d4490e" class="">Compute row <link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">i=5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">5</span></span></span></span></span><span>﻿</span></span> for an embedding dimension of <link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">d=3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">3</span></span></span></span></span><span>﻿</span></span>, with max sequence length of <link rel="stylesheet" type="text/css" href="../../notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">C=10</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">10</span></span></span></span></span><span>﻿</span></span>. </p><figure id="14d2e7d3-01e5-80f1-8d1f-c9495fed5f27" class="image" style="text-align:center"><a href="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-29_at_17.10.11.png"><img style="width:432px" src="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-29_at_17.10.11.png"/></a></figure><p id="14d2e7d3-01e5-80ad-a693-f6f1b625de41" class="">
</p></details></li></ul><hr id="14d2e7d3-01e5-8017-9dc3-cd01d46471ef"/><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="15f2e7d3-01e5-806e-90a1-cb1ae92934c3"><div style="font-size:1.5em"><img class="icon" src="https://www.notion.so/icons/exclamation-mark_red.svg"/></div><div style="width:100%"><p id="eb9d37db-7ece-414a-ad4c-7d7a71666b7c" class=""><strong>Problem</strong></p><p id="672c9e2e-1625-4302-81bb-e8bcef63a635" class="">Limiting the model to calculating similarity in <span style="border-bottom:0.05em solid">just one way</span> is restrictive </p><ul id="15f2e7d3-01e5-8067-bdb7-cd56030943d6" class="toggle"><li><details open=""><summary><mark class="highlight-purple">Example</mark></summary><p id="14d2e7d3-01e5-806c-bc66-cff4af820d89" class="">e.g. “the cat chased the mouse down the dark alley” </p><ul id="14d2e7d3-01e5-80e8-a92c-c8bb4167a64b" class="bulleted-list"><li style="list-style-type:disc">Semantic Similarity: “cat” ↔ “mouse” are similar; both animals. </li></ul><ul id="14d2e7d3-01e5-80c8-bb47-d563e1f9889f" class="bulleted-list"><li style="list-style-type:disc">Positional Similarity: “The” ↔ “cat” are similar because they’re close. </li></ul><ul id="14d2e7d3-01e5-8009-9b85-cca34f45692d" class="bulleted-list"><li style="list-style-type:disc">Subject/Verb Similarity: “Cat” ↔ “chased” since the verb applies to it. </li></ul></details></li></ul><p id="eee442e3-11f8-4ee7-b402-9c62499e1349" class=""><em><mark class="highlight-gray"><strong>Solution: </strong></mark></em><em><mark class="highlight-gray">Multi-Head Attention</mark></em></p></div></figure><h3 id="14d2e7d3-01e5-8043-bcd7-f60250595b17" class="">Multi-Head Attention </h3><p id="14d2e7d3-01e5-807a-8f41-da0706713457" class="">Uses multiple attention-mechanisms in parallel. </p><ul id="15f2e7d3-01e5-8017-8945-c8067f00eab1" class="bulleted-list"><li style="list-style-type:disc"><strong>why? </strong>better captures subtle and long-range dependencies </li></ul><figure id="14d2e7d3-01e5-80d7-b8d9-eb8becd939b6" class="image"><a href="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-29_at_17.14.05.png"><img style="width:192px" src="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-29_at_17.14.05.png"/></a></figure><ul id="14d2e7d3-01e5-8047-b241-fc2962980659" class="bulleted-list"><li style="list-style-type:disc">Split (K,Q,V) into multiple parts, and apply multiple <strong>attention-heads</strong></li></ul><ul id="14d2e7d3-01e5-80e7-a144-fd34fd2579e8" class="bulleted-list"><li style="list-style-type:disc">Each <strong>head</strong> has its own set of <strong>learned weights</strong>, and works independently, focusing on its own version of similarity. </li></ul><ul id="14d2e7d3-01e5-803e-a3b5-c4613051d9ac" class="bulleted-list"><li style="list-style-type:disc">The separate outputs are then <strong>concatenated</strong> and put back into the full dimension. </li></ul><hr id="14d2e7d3-01e5-8059-8462-dd76f064bd22"/><h3 id="14d2e7d3-01e5-80a0-af98-c11b8dab4074" class="">Transformer NN </h3><ul id="15f2e7d3-01e5-802a-b46b-c41a86641137" class="bulleted-list"><li style="list-style-type:disc"><em>Attention + Positional Encoding + Multi-Head Attention</em></li></ul><ul id="15f2e7d3-01e5-8012-8546-dab04126a5e1" class="bulleted-list"><li style="list-style-type:disc">Let us work with sequences, without having to process one-at-a-time (sequentially)</li></ul><ul id="15f2e7d3-01e5-8024-b6e6-c18a35f3a756" class="bulleted-list"><li style="list-style-type:disc">Captures <em>fine grained</em> &amp; <em>broad relationships</em>. </li></ul><figure id="14d2e7d3-01e5-80ba-8ce0-ec081ce57be5" class="image"><a href="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-29_at_17.22.05.png"><img style="width:576px" src="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-29_at_17.22.05.png"/></a></figure><ul id="14d2e7d3-01e5-80d8-83dc-c9660a5932f6" class="bulleted-list"><li style="list-style-type:disc"><strong>Encoder</strong> <em><mark class="highlight-gray">(only 1 layer shown, can have many stacked)</mark></em><mark class="highlight-gray">:</mark> <ul id="14d2e7d3-01e5-8063-add8-e3b54d188235" class="bulleted-list"><li style="list-style-type:circle">input ⇒ <strong>attention based representation</strong></li></ul><ul id="14d2e7d3-01e5-80ef-8b54-c7b516cc9c21" class="bulleted-list"><li style="list-style-type:circle">2 sub-layers: <ol type="1" id="14d2e7d3-01e5-80e4-845d-e8fa079c3c1a" class="numbered-list" start="1"><li>multi-head, self-attention layer</li></ol><ol type="1" id="14d2e7d3-01e5-8080-b6a3-e9a254eac3c5" class="numbered-list" start="2"><li>feedforward layer: point wise (same transformation for each seq position, indep.) </li></ol></li></ul><ul id="14d2e7d3-01e5-8027-ade7-d9428a46965f" class="bulleted-list"><li style="list-style-type:circle">Each has add/norm to keep the network stable. </li></ul></li></ul><ul id="14d2e7d3-01e5-80d0-b05b-ee6767342a86" class="bulleted-list"><li style="list-style-type:disc"><strong>Decoder</strong><ul id="14d2e7d3-01e5-80fd-b92c-dca730798f04" class="bulleted-list"><li style="list-style-type:circle">Generates output sequence using info from encoder and own previous outputs. </li></ul><ul id="14d2e7d3-01e5-8076-a968-ccaee61d4c63" class="bulleted-list"><li style="list-style-type:circle">Predicts one word at a time (first, second based on first,…)</li></ul><ul id="14d2e7d3-01e5-8065-8b7d-dfe653b1aff7" class="bulleted-list"><li style="list-style-type:circle">Has a <strong>masked multi-head-attention layer</strong><ul id="14d2e7d3-01e5-8019-94c5-e36660c912c9" class="bulleted-list"><li style="list-style-type:square">masked ⇒ each position only attends to earlier positions</li></ul><ul id="14d2e7d3-01e5-8075-977f-eb8286cfb8d5" class="bulleted-list"><li style="list-style-type:square">why? because during training, full sequence is available to model. We want to prevent it from using future tokens to predict the current one. </li></ul></li></ul><ul id="14d2e7d3-01e5-804b-92c3-e73a045e9ed2" class="bulleted-list"><li style="list-style-type:circle">Multi-head attention layer uses encoders to output ⇒ allowing it to draw context to the input sequence. </li></ul></li></ul><ul id="15f2e7d3-01e5-80e8-92a0-ed693bad02fd" class="bulleted-list"><li style="list-style-type:disc"><strong>Summary</strong> — does self-attention in 3 ways) <figure id="15f2e7d3-01e5-8044-a91f-eb77a1b93461" class="image" style="text-align:center"><a href="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-29_at_17.24.00.png"><img style="width:432px" src="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-29_at_17.24.00.png"/></a></figure><ul id="15f2e7d3-01e5-8052-9210-c40f63f8a496" class="bulleted-list"><li style="list-style-type:circle">(1) Multi-Head Attention (in encoder): connect all positions</li></ul><ul id="15f2e7d3-01e5-8083-8314-d10dc476b1ea" class="bulleted-list"><li style="list-style-type:circle">(2) Self-Attention Layers (decoder): handle past info only</li></ul><ul id="15f2e7d3-01e5-8067-b822-d4fea2656099" class="bulleted-list"><li style="list-style-type:circle">(3) Multi-Head Attention (encoder↔decoder): <ul id="15f2e7d3-01e5-80f0-abc0-cada7679b0d9" class="bulleted-list"><li style="list-style-type:square">decoder queries the encoder outputs, to draw context to the input sequence. </li></ul></li></ul></li></ul><hr id="14d2e7d3-01e5-80ef-a95a-ce502b542f17"/><h2 id="14d2e7d3-01e5-8022-a73e-dff211b02305" class="">GPT</h2><p id="15f2e7d3-01e5-8037-9fdd-fb53cc0e35dd" class=""><em><mark class="highlight-gray"><strong>G</strong></mark></em><em><mark class="highlight-gray">enerative</mark></em><em><mark class="highlight-gray"><strong> P</strong></mark></em><em><mark class="highlight-gray">re-Training</mark></em><em><mark class="highlight-gray"><strong> T</strong></mark></em><em><mark class="highlight-gray">ransformer</mark></em></p><figure id="15f2e7d3-01e5-8075-babb-d4d5890fb60c" class="image"><a href="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-12-17_at_14.06.50.png"><img style="width:144px" src="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-12-17_at_14.06.50.png"/></a></figure><p id="14d2e7d3-01e5-800f-99f9-fa2f35eb809b" class="">2 Phases: </p><ol type="1" id="15f2e7d3-01e5-80f9-b8cb-eae7291d0c5c" class="numbered-list" start="1"><li><span style="border-bottom:0.05em solid">Unsupervised</span> Pre-Training: <ul id="15f2e7d3-01e5-8026-8556-c304c555f22e" class="bulleted-list"><li style="list-style-type:disc">Model given lots of raw text</li></ul><ul id="15f2e7d3-01e5-8024-83e1-e2e137009129" class="bulleted-list"><li style="list-style-type:disc">Learns to predict next word in sequence</li></ul></li></ol><ol type="1" id="15f2e7d3-01e5-8032-94af-fd36f73b9426" class="numbered-list" start="2"><li><span style="border-bottom:0.05em solid">Supervised</span> Fine Tuning <ul id="15f2e7d3-01e5-80a6-8a5b-e8f9e5b0f442" class="bulleted-list"><li style="list-style-type:disc">Learns to predict the <span style="border-bottom:0.05em solid">label</span> (focused on <span style="border-bottom:0.05em solid">specifically task</span>) </li></ul><ul id="15f2e7d3-01e5-8034-8025-e0c6de857ff9" class="bulleted-list"><li style="list-style-type:disc">Balanced with phase 1 <em><mark class="highlight-gray">(ie: a </mark></em><em><mark class="highlight-gray"><span style="border-bottom:0.05em solid">generalized</span></mark></em><em><mark class="highlight-gray"> prediction, based on language). </mark></em></li></ul></li></ol><p id="15f2e7d3-01e5-8037-8524-c3cfc3506ef8" class="">Result ⇒ Effective at <strong>general tasks</strong>, even if not specifically trained on them. </p><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="15f2e7d3-01e5-80d2-9c97-f063d60b0bf1"><div style="font-size:1.5em"><img class="icon" src="https://www.notion.so/icons/question-mark_gray.svg"/></div><div style="width:100%"><p id="e641e183-9430-4f17-b998-020f55265df6" class=""><strong>In-Context Learning </strong></p><p id="15f2e7d3-01e5-801a-9511-c882e59b19d3" class="">Adapts <mark class="highlight-gray">(”learns” in a way) </mark>to tasks based a <strong>prompt</strong>, without a <strong>gradient update</strong> (don’t need to always retrain the model for a new input, uses <strong>context</strong>)</p><ul id="15f2e7d3-01e5-80f2-8d51-cbef48895a69" class="toggle"><li><details open=""><summary><mark class="highlight-purple">Example</mark></summary><figure id="15f2e7d3-01e5-80ee-b5ac-fdf7e8713103" class="image"><a href="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-12-17_at_14.12.19.png"><img style="width:2484px" src="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-12-17_at_14.12.19.png"/></a></figure></details></li></ul></div></figure><hr id="14d2e7d3-01e5-8031-9765-df256889e8df"/><h3 id="14d2e7d3-01e5-80b4-b35a-ce43b088fbd6" class="">Drawbacks of Attention-Based models </h3><ol type="1" id="14d2e7d3-01e5-803f-9193-e30c08b1ba4d" class="numbered-list" start="1"><li>Self Attention is Computationally Expensive <ul id="14d2e7d3-01e5-80a5-8f64-c85472dc5899" class="bulleted-list"><li style="list-style-type:disc"><em>“Positions all attend to each-other”</em> unlike an RNN which only attends to the previous. </li></ul></li></ol><figure id="14d2e7d3-01e5-8025-9894-c8167987ce5e" class="image"><a href="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-29_at_17.27.45.png"><img style="width:336px" src="L16%20%E2%80%94%20Attention%20Mechanisms/Screenshot_2024-11-29_at_17.27.45.png"/></a></figure><p id="14d2e7d3-01e5-805c-bf54-f4f5378c4bbb" class="">
</p><p id="14d2e7d3-01e5-8035-8fea-e088c3529c41" class="">
</p><p id="14d2e7d3-01e5-80e9-a8ba-c1e0632183a7" class="">
</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>