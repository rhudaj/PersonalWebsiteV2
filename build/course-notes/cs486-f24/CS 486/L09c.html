<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>L09c</title><link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/></head><body><article id="1522e7d3-01e5-8024-b012-f66a37dcabc7" class="page sans"><header><div class="page-header-icon undefined"><img class="icon" src="https://www.notion.so/icons/document_pink.svg"/></div><h1 class="page-title">L09c</h1><p class="page-description"></p></header><div class="page-body"><p id="1522e7d3-01e5-8044-956a-de3af8912848" class=""><strong>Topic: </strong>Reinforcement Learning </p><hr id="1522e7d3-01e5-80af-8829-d0d350552a89"/><p id="1522e7d3-01e5-80ae-bec5-f958f749aac7" class=""><strong>What is it? </strong></p><ul id="1522e7d3-01e5-802c-b341-ce40165f9414" class="bulleted-list"><li style="list-style-type:disc">An agent that learns how to act while acting at the same time. </li></ul><ul id="1522e7d3-01e5-803e-a2ad-ff3a0a998a3b" class="bulleted-list"><li style="list-style-type:disc">Decision-theoretic planning, but model of dynamics and reward aren’t given. And we still want to maximize rewards. </li></ul><ul id="1522e7d3-01e5-8094-86d1-f35d0ae6eb3c" class="bulleted-list"><li style="list-style-type:disc">The agent knows: <ul id="1522e7d3-01e5-80d0-a2be-f5a2afd5d3e4" class="bulleted-list"><li style="list-style-type:circle">the possible states and actions. </li></ul><ul id="1522e7d3-01e5-8058-a47c-f5c34afa8f8c" class="bulleted-list"><li style="list-style-type:circle">(<em>fully observable world) </em>What current state its in, what immediate reward it received </li></ul></li></ul><hr id="1522e7d3-01e5-80c0-bfe7-ca61df560b69"/><p id="1522e7d3-01e5-8072-8381-cc4ef491094a" class="">The agent goes through a <span style="border-bottom:0.05em solid">sequence</span> of <span style="border-bottom:0.05em solid">experiences</span>: <div class="indented"><p id="1522e7d3-01e5-80f1-b7db-ca0b98394bba" class="">(state, action, reward), (state, action, reward), (state, action, reward), … </p></div></p><p id="1522e7d3-01e5-8044-bc56-eee69d43557b" class=""><strong>Key Tradeoff:</strong></p><p id="1522e7d3-01e5-8090-8169-fca3c5c1e1e1" class="">At each step the agent must decide wether to: </p><ol type="1" id="1522e7d3-01e5-803c-a181-ef02f98bde2c" class="numbered-list" start="1"><li><strong>Explore</strong> — to gain more knowledge</li></ol><ol type="1" id="1522e7d3-01e5-808b-8b15-c679a537678d" class="numbered-list" start="2"><li><strong>Exploit</strong> — the knowledge it already has (be greedy) </li></ol><p id="1522e7d3-01e5-808f-8ff7-de6b222badc4" class=""><strong>Key Challenge: </strong></p><ul id="1522e7d3-01e5-80ce-bf19-f1a6a5593e98" class="bulleted-list"><li style="list-style-type:disc">The agent is not aware of long-term effects/rewards right away. </li></ul><hr id="1522e7d3-01e5-8027-8efd-ebdefaca2c0c"/><p id="1522e7d3-01e5-805d-ba11-d4d37ac08c55" class=""><strong>2 Main Approaches </strong></p><ol type="1" id="1522e7d3-01e5-807e-ac1b-c0e1615fd904" class="numbered-list" start="1"><li><strong>Model Based</strong></li></ol><ul id="1522e7d3-01e5-8037-a3f8-ce3b5a86d555" class="bulleted-list"><li style="list-style-type:disc">The agent explores randomlly until it learns the model: <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mi mathvariant="normal">∣</mi><mi>a</mi><mo separator="true">,</mo><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(s&#x27;|a,s)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span> and <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">R(s,a,s&#x27;)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span></li></ul><ul id="1522e7d3-01e5-8096-bdff-f5f384849395" class="bulleted-list"><li style="list-style-type:disc">i.e. You have an empty MDP (no tables filled out), get the dataset first. Then with the dataset you can get the model parameters (e.g. w’ value iteration).</li></ul><ul id="1522e7d3-01e5-80b9-a049-f148e38cc35f" class="bulleted-list"><li style="list-style-type:disc"><span style="border-bottom:0.05em solid">Problem:</span> while its acting randomly, its not maximizing its rewards at the same time </li></ul><ol type="1" id="1522e7d3-01e5-80e1-a931-e86a52d8b37f" class="numbered-list" start="2"><li><mark class="highlight-red"><strong>Model-Free</strong></mark></li></ol><ul id="1522e7d3-01e5-8023-8690-c73307c6e0c8" class="bulleted-list"><li style="list-style-type:disc">Learn <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Q</mi><mo>∗</mo></msup><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Q^*(s,a)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6887em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span> while also using it to guide your actions. </li></ul><hr id="1522e7d3-01e5-80d9-afbf-d5e0143fbfa9"/><h2 id="1522e7d3-01e5-8047-a22c-c23d60d30930" class="">Model-Free Learner</h2><p id="1522e7d3-01e5-800d-8620-edcc9b00ee5e" class=""><strong>Q-Learning</strong> is <span style="border-bottom:0.05em solid">Asynchronous-Value-Update</span>, but using the experience of the agent (<strong>empirical probabilities and rewards</strong>) instead of using the given transition and reward function. </p><p id="1522e7d3-01e5-80c8-beb1-e7083b12813e" class="">So its the same formulas but using empirical transitions. </p><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="1522e7d3-01e5-8023-8915-fe27d4887ce1"><div style="font-size:1.5em"><img class="icon" src="https://www.notion.so/icons/exclamation-mark_gray.svg"/></div><div style="width:100%"><p id="2aa6325b-3994-477e-932e-70fc34f18b23" class=""><strong>Q-Learning </strong>converges to the <strong>optimal policy </strong>in the limit — the agent tries each action in each stage (all s, a) infinitely often. </p></div></figure><ul id="1522e7d3-01e5-8065-a79e-c4c8e167acb4" class="toggle"><li><details open=""><summary><mark class="highlight-purple"><strong>Example</strong></mark><strong> </strong><mark class="highlight-purple"><strong>— Student Bot </strong></mark></summary><p id="1522e7d3-01e5-80dc-a058-e310b0f3564e" class="">Recall: tradeoff between short and long-term rewards. </p><ul id="1522e7d3-01e5-80e9-b275-f0b420dac8e0" class="bulleted-list"><li style="list-style-type:disc">After 100,000 iterations, the agent still always chooses to party (to reap the short-term rewards). </li></ul><ul id="1522e7d3-01e5-80ae-b81b-f8a090c9c037" class="bulleted-list"><li style="list-style-type:disc">After 200,000 iterations, the agent figures out that studying has long-term rewards, so it starts to do that too. </li></ul></details></li></ul><hr id="1522e7d3-01e5-8066-9207-c1ae9c4938e8"/><h3 id="1522e7d3-01e5-80fe-8714-eabc7c9d752f" class="">Off/On-Policy Learning</h3><p id="1522e7d3-01e5-8013-9166-c1dd84fe855f" class="">In Q-Learning, we used the <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">ϵ</span></span></span></span></span><span>﻿</span></span>-greedy policy: <div class="indented"><p id="1522e7d3-01e5-808f-b03b-e96f1e052608" class="">Act greedy 1-<link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">ϵ</span></span></span></span></span><span>﻿</span></span> % of the time, and randomly <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">ϵ</span></span></span></span></span><span>﻿</span></span> % of the time. </p></div></p><p id="1522e7d3-01e5-80ea-93bf-f9f61ee8357c" class=""><strong>Off Policy (Q-Learning): </strong>what the agent learns, is not what it does. <div class="indented"><p id="1522e7d3-01e5-8038-a765-d79d2d39112f" class="">In Q-Learning (<strong>off policy</strong>), your policy (ie: greedy part of the exploration policy) can be anything, event totally random. However, you are are still finding the <strong>optimal policy</strong>. This is because after each experience, you update <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span></span><span>﻿</span></span> by maximizing over all actions in the next state. So even the agent does a random action everytime, your still learning the value of the optimal policy: <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Q</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">Q^*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8831em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6887em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span>. </p><p id="1522e7d3-01e5-8003-b635-f2ef8ee238e9" class="">⇒ Q is an estimate of the <strong>optimal policy</strong>, but this is NOT the <strong>actual policy</strong> being followed. The actual policy being followed has an exploration factor. </p></div></p><p id="1522e7d3-01e5-80ab-8a87-fd9f9ba4876e" class=""><strong>On-Policy (SARSA): </strong>the agent learns the value of the policy being followed. <div class="indented"><p id="1522e7d3-01e5-80a7-9499-cfeb4f364925" class=""><link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span></span><span>﻿</span></span> is the value of the policy being followed (e.g. <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">ϵ</span></span></span></span></span><span>﻿</span></span>-greedy) </p></div></p><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="1522e7d3-01e5-805b-b9a6-e93ac8ea5207"><div style="font-size:1.5em"><img class="icon" src="https://www.notion.so/icons/exclamation-mark_gray.svg"/></div><div style="width:100%"><p id="d0d32d85-4e4a-4633-8495-f26822fcc8ce" class="">Recall, the optimal policy is only reached in the limit. So you ever really get there. SARSA adresses this by saying: “<strong>might as well learn the value of the policy that includes the exploration bonus</strong>”. So SARSA takes into account the value, or potential harm that the random exploration part of the policy might bring. <div class="indented"><p id="1522e7d3-01e5-8047-b296-c073edc63d17" class="">e.g. SARSA might have taken a random action once and got to a random state that turned out to be very bad. It will take this “value” into account through updating <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span></span><span>﻿</span></span>.</p></div></p></div></figure><h3 id="1522e7d3-01e5-804d-b15f-ef935cf47180" class="">Summary</h3><figure id="1522e7d3-01e5-80b6-a582-dd2e75277684" class="image"><a href="L09c/IMG_2F5E962B33BE-1.jpeg"><img style="width:528px" src="L09c/IMG_2F5E962B33BE-1.jpeg"/></a></figure><hr id="1522e7d3-01e5-8052-b2f6-f32946a51e8b"/><h2 id="1522e7d3-01e5-80e6-a20d-f9af0eabc0c2" class="">Model-Based-Learner</h2><p id="1522e7d3-01e5-80c5-9eae-fcf1057fb093" class="">Learn the MDP</p><ul id="1522e7d3-01e5-8018-a2e4-d8761ab0be16" class="bulleted-list"><li style="list-style-type:disc">Go out and act randomly. </li></ul><ul id="1522e7d3-01e5-8027-a66f-fb818ff3f05a" class="bulleted-list"><li style="list-style-type:disc">After each experience, update <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span></span></span></span><span>﻿</span></span> and <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span></span></span></span><span>﻿</span></span> functions, then do steps of Asynch-VI. </li></ul><ul id="1522e7d3-01e5-8094-82a5-ddd90af61a25" class="bulleted-list"><li style="list-style-type:disc">Then solve for optimal policy. </li></ul><p id="1522e7d3-01e5-801d-991a-d946006cde99" class="">Algorithm: skipped </p><hr id="1522e7d3-01e5-8086-a4eb-fc007817784f"/><p id="1522e7d3-01e5-80a6-9504-e5741f92bfef" class="">Problem: for large state spaces, <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span></span><span>﻿</span></span> is large, and these methods take too long</p><p id="1522e7d3-01e5-804f-a0dd-efc77bd75982" class="">Solution: </p><h2 id="1522e7d3-01e5-80c5-813f-c5e932028ce6" class="">Aproximating <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span></span><span>﻿</span></span></h2><p id="1522e7d3-01e5-801e-a4b5-e622b7b357ce" class="">Consider states as factors of features: </p><figure id="1522e7d3-01e5-8088-99d9-cf297635d8a0" class="equation"><link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>s</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">s=(x_1,\dots, x_n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></div></figure><p id="1522e7d3-01e5-80c0-a0d4-d4da90c86613" class="">Then approximate <link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Q(s,a)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span> as a <strong>weighted sum</strong> of the features. The goal is now to find these weights. </p><ol type="1" id="1522e7d3-01e5-80ed-bf96-d582fd85a488" class="numbered-list" start="1"><li><strong>Linear</strong></li></ol><figure id="1522e7d3-01e5-80ea-b0e3-efef5cad65f6" class="equation"><link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>Q</mi><mi>w</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>≈</mo><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>w</mi><mrow><mi>a</mi><mi>i</mi></mrow></msub><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">Q_w(s,a) \approx \sum_i w_{ai} x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.3277em;vertical-align:-1.2777em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ai</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></div></figure><ol type="1" id="1522e7d3-01e5-80ec-aaf3-f629e66a811e" class="numbered-list" start="2"><li><strong>Non-Linear </strong>(e.g. NN) </li></ol><figure id="1522e7d3-01e5-808d-a7b5-e994ef8747c6" class="equation"><link rel="stylesheet" type="text/css" href="/course-notes/notion_styles.css"/><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>Q</mi><mi>w</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>≈</mo><mi>g</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo separator="true">;</mo><mi mathvariant="bold">w</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Q_w(s,a) \approx g(\bold x;\bold{w})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathbf" style="margin-right:0.01597em;">w</span><span class="mclose">)</span></span></span></span></span></div></figure><ul id="1522e7d3-01e5-8083-95c1-efd92b33dc9d" class="bulleted-list"><li style="list-style-type:disc">inputs: experiences (states, actions)</li></ul><ul id="1522e7d3-01e5-80d2-ad17-fb63e0bb6c5c" class="bulleted-list"><li style="list-style-type:disc">outputs: difference between current estimate and new estimate.  </li></ul><p id="1522e7d3-01e5-80d1-9c71-cbe75f7e04a3" class="">
</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>